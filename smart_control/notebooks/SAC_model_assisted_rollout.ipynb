{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "nQnmcm0oI1Q-"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vekhJpsOxLK"
   },
   "source": [
    "# SBSim: A tutorial of using Reinforcement Learning for Optimizing Energy Use and Minimizing Carbon Emission in Office Buildings\n",
    "\n",
    "___\n",
    "\n",
    "Commercial office buildings contribute 17 percent of Carbon Emissions in the US, according to the US Energy Information Administration (EIA), and improving their efficiency will reduce their environmental burden and operating cost. A major contributor of energy consumption in these buildings are the Heating, Ventilation, and Air Conditioning (HVAC) devices. HVAC devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) agent is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many practical challenges. Most existing work on applying RL to this important task either makes use of proprietary data, or focuses on expensive and proprietary simulations that may not be grounded in the real world. We present the Smart Buildings Control Suite, the first open source interactive HVAC control dataset extracted from live sensor measurements of devices in real office buildings. The dataset consists of two components: real-world historical data from two buildings, for offline RL, and a lightweight interactive simulator for each of these buildings, calibrated using the historical data, for online and model-based RL. For ease of use, our RL environments are all compatible with the OpenAI gym environment standard. We believe this benchmark will accelerate progress and collaboration on HVAC optimization.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook accompanies the paper titled, **Real-World Data and Calibrated Simulation Suite for Offline Training of Reinforcement Learning Agents to Optimize Energy and Emission in Office Buildings** by Judah Goldfeder and John Sipple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "YchP7JXbSXS1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 17:52:25.677112: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 17:52:25.681007: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 17:52:25.719347: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 17:52:25.719380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 17:52:25.720659: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 17:52:25.727915: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 17:52:25.728751: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 17:52:26.908099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# @title Imports\n",
    "from dataclasses import dataclass\n",
    "import datetime, pytz\n",
    "import enum\n",
    "import functools\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import time\n",
    "from typing import Final, Sequence\n",
    "from typing import Optional\n",
    "from typing import Union, cast\n",
    "os.environ['WRAPT_DISABLE_EXTENSIONS'] = 'true'\n",
    "from absl import logging\n",
    "import gin\n",
    "from matplotlib import patches\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import reverb\n",
    "import mediapy as media\n",
    "from IPython.display import clear_output\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/burg/home/ssa2206/sbsim_dual_control/smart_control/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_control.environment import environment\n",
    "\n",
    "from smart_control.proto import smart_control_building_pb2, smart_control_normalization_pb2\n",
    "from smart_control.reward import electricity_energy_cost, natural_gas_energy_cost, setpoint_energy_carbon_reward, setpoint_energy_carbon_regret\n",
    "\n",
    "from smart_control.simulator import randomized_arrival_departure_occupancy, rejection_simulator_building\n",
    "from smart_control.simulator import simulator_building, step_function_occupancy, stochastic_convection_simulator\n",
    "\n",
    "from smart_control.utils import bounded_action_normalizer, building_renderer, controller_reader\n",
    "from smart_control.utils import controller_writer, conversion_utils, observation_normalizer, reader_lib\n",
    "from smart_control.utils import writer_lib, histogram_reducer, environment_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.agents.sac import sac_agent, tanh_normal_projection_network\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.networks import nest_map, sequential\n",
    "from tf_agents.policies import greedy_policy, py_tf_eager_policy, random_py_policy, tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer, reverb_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.train import actor, learner, triggers\n",
    "from tf_agents.train.utils import spec_utils, train_utils\n",
    "from tf_agents.trajectories import policy_step, StepType\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory as trajectory_lib\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.typing import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_loader import load_envs\n",
    "from plot_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "sDDU5FmLkYo-"
   },
   "outputs": [],
   "source": [
    "# @title Set local runtime configurations\n",
    "\n",
    "\n",
    "def logging_info(*args):\n",
    "    logging.info(*args)\n",
    "    print(*args)\n",
    "\n",
    "data_path = \"/burg/home/ssa2206/sbsim_dual_control/smart_control/configs/resources/sb1/\" #@param {type:\"string\"}\n",
    "metrics_path = \"/burg/home/ssa2206/sbsim_dual_control/metrics/\" #@param {type:\"string\"}\n",
    "output_data_path = '/burg/home/ssa2206/sbsim_dual_control/output' #@param {type:\"string\"}\n",
    "root_dir = \"/burg/home/ssa2206/sbsim_dual_control/root\" #@param {type:\"string\"}\n",
    "\n",
    "!mkdir -p $root_dir\n",
    "!mkdir -p $output_data_path\n",
    "!mkdir -p $metrics_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remap_filepath(filepath) -> str:\n",
    "    return str(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting Utities\n",
    "reward_shift = 0\n",
    "reward_scale = 1.0\n",
    "person_productivity_hour = 300.0\n",
    "time_zone = \"US/Pacific\"\n",
    "\n",
    "KELVIN_TO_CELSIUS = 273.15\n",
    "\n",
    "def render_env(env: environment.Environment):\n",
    "    \"\"\"Renders the environment.\"\"\"\n",
    "    building_layout = env.building._simulator._building._floor_plan\n",
    "\n",
    "    # create a renderer\n",
    "    renderer = building_renderer.BuildingRenderer(building_layout, 1)\n",
    "\n",
    "    # get the current temps to render\n",
    "    # this also is not ideal, since the temps are not fully exposed.\n",
    "    # V Ideally this should be a publicly accessable field\n",
    "    temps = env.building._simulator._building.temp\n",
    "\n",
    "    input_q = env.building._simulator._building.input_q\n",
    "\n",
    "    # render\n",
    "    vmin = 285\n",
    "    vmax = 305\n",
    "    image = renderer.render(temps, cmap='bwr', vmin=vmin, vmax=vmax, colorbar=False, \n",
    "                            input_q=input_q, diff_range=0.5, diff_size=1,).convert('RGB')\n",
    "    media.show_image(image, title='Environment %s' % env.current_simulation_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/burg/home/ssa2206/sbsim_dual_control/smart_control/configs/resources/sb1/sim_config.gin\n",
      "/burg/home/ssa2206/sbsim_dual_control/smart_control/configs/resources/sb1/sim_config.gin\n"
     ]
    }
   ],
   "source": [
    "eval_env, collect_env, initial_collect_env = load_envs(data_path, metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<smart_control.simulator.simulator_building.SimulatorBuilding at 0x1554cef04850>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env.building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/burg/home/ssa2206/sbsim_dual_control/metrics'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env._metrics_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c55CehnYR8lY"
   },
   "source": [
    "In the section below, we'll define a function that accepts the envirnment and a policy, and runs a fixed number of episodes. The policy can be a rules-based policy or an RL-based policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "bitzHo5_UbXy"
   },
   "outputs": [],
   "source": [
    "# @title Define a method to execute the policy on the environment.\n",
    "\n",
    "\n",
    "def get_trajectory(time_step, current_action: policy_step.PolicyStep):\n",
    "    \"\"\"Get the trajectory for the current action and time step.\"\"\"\n",
    "    observation = time_step.observation\n",
    "    action = current_action.action\n",
    "    policy_info = ()\n",
    "    reward = time_step.reward\n",
    "    discount = time_step.discount\n",
    "\n",
    "    if time_step.is_first():\n",
    "        return(trajectory.first(observation, action, policy_info, reward, discount))\n",
    "    elif time_step.is_last():\n",
    "        return(trajectory.last(observation, action, policy_info, reward, discount))\n",
    "    else:\n",
    "        return(trajectory.mid(observation, action, policy_info, reward, discount))\n",
    "\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=1, time_zone: str = \"US/Pacific\", \n",
    "                       render_interval_steps: int = 24,trajectory_observers=None,):\n",
    "    \"\"\"Computes the average return of the policy on the environment.\n",
    "    Args:\n",
    "    environment: environment.Environment\n",
    "    policy: policy.Policy\n",
    "    num_episodes: total number of eposides to run.\n",
    "    time_zone: time zone of the environment\n",
    "    render_interval_steps: Number of steps to take between rendering.\n",
    "    trajectory_observers: list of trajectory observers for use in rendering.\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        t0 = time.time()\n",
    "        epoch = t0\n",
    "        step_id = 0\n",
    "        execution_times = []\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "\n",
    "            if trajectory_observers is not None:\n",
    "                traj = get_trajectory(time_step, action_step)\n",
    "                for observer in trajectory_observers:\n",
    "                    observer(traj)\n",
    "\n",
    "            episode_return += time_step.reward\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            episode_seconds = t1 - epoch\n",
    "            execution_times.append(dt)\n",
    "            sim_time = environment.current_simulation_timestamp.tz_convert(time_zone)\n",
    "\n",
    "            print(\"Step %5d Sim Time: %s, Reward: %8.2f, Return: %8.2f, Mean Step Time:\"\n",
    "                  \" %8.2f s, Episode Time: %8.2f s\" % (step_id, sim_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                                                       time_step.reward, episode_return, \n",
    "                                                       np.mean(execution_times), episode_seconds,)\n",
    "                 )\n",
    "            if (step_id > 0) and (step_id % render_interval_steps == 0):\n",
    "                if environment._metrics_path:\n",
    "                    clear_output(wait=True)\n",
    "                    reader = get_latest_episode_reader(environment._metrics_path)\n",
    "                    plot_timeseries_charts(reader, time_zone)\n",
    "                render_env(environment)\n",
    "\n",
    "            t0 = t1\n",
    "            step_id += 1\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAYOf5Xtzi2u"
   },
   "source": [
    "Next, we will run the static control setpoints on the environment to establish baseline performance.\n",
    "\n",
    "**Note:** This will take some time to execute. Feel free to skip this step if you want to jump directly to the RL section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "3Zv-lSiWDp50"
   },
   "outputs": [],
   "source": [
    "# @title Optionally, execute the schedule policy on the environment\n",
    "# Optional\n",
    "#compute_avg_return(eval_env, schedule_policy, 1, time_zone=\"US/Pacific\", render_interval_steps=144, trajectory_observers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "dJ_EMQkZdw8q"
   },
   "outputs": [],
   "source": [
    "# @title Define Observers\n",
    "class RenderAndPlotObserver:\n",
    "    \"\"\"Renders and plots the environment.\"\"\"\n",
    "    def __init__(self, render_interval_steps: int = 10, environment=None,):\n",
    "        self._counter = 0\n",
    "        self._render_interval_steps = render_interval_steps\n",
    "        self._environment = environment\n",
    "        self._cumulative_reward = 0.0\n",
    "        self._start_time = None\n",
    "        if self._environment is not None:\n",
    "            self._num_timesteps_in_episode = (self._environment._num_timesteps_in_episode)\n",
    "            self._environment._end_timestamp\n",
    "\n",
    "    def __call__(self, trajectory: trajectory_lib.Trajectory) -> None:\n",
    "        reward = trajectory.reward\n",
    "        self._cumulative_reward += reward\n",
    "        self._counter += 1\n",
    "        if self._start_time is None:\n",
    "            self._start_time = pd.Timestamp.now()\n",
    "\n",
    "        if self._counter % self._render_interval_steps == 0 and self._environment:\n",
    "            execution_time = pd.Timestamp.now() - self._start_time\n",
    "            mean_execution_time = execution_time.total_seconds() / self._counter\n",
    "            clear_output(wait=True)\n",
    "            if self._environment._metrics_path is not None:\n",
    "                reader = get_latest_episode_reader(self._environment._metrics_path)\n",
    "                plot_timeseries_charts(reader, time_zone)\n",
    "\n",
    "            render_env(self._environment)\n",
    "\n",
    "class PrintStatusObserver:\n",
    "    \"\"\"Prints status information.\"\"\"\n",
    "\n",
    "    def __init__(self, status_interval_steps: int = 1, environment=None, replay_buffer=None):\n",
    "        self._counter = 0\n",
    "        self._status_interval_steps = status_interval_steps\n",
    "        self._environment = environment\n",
    "        self._cumulative_reward = 0.0\n",
    "        self._replay_buffer = replay_buffer\n",
    "\n",
    "        self._start_time = None\n",
    "        if self._environment is not None:\n",
    "            self._num_timesteps_in_episode = (\n",
    "                    self._environment._num_timesteps_in_episode\n",
    "            )\n",
    "            self._environment._end_timestamp\n",
    "\n",
    "    def __call__(self, trajectory: trajectory_lib.Trajectory) -> None:\n",
    "\n",
    "        reward = trajectory.reward\n",
    "        self._cumulative_reward += reward\n",
    "        self._counter += 1\n",
    "        if self._start_time is None:\n",
    "            self._start_time = pd.Timestamp.now()\n",
    "\n",
    "        if self._counter % self._status_interval_steps == 0 and self._environment:\n",
    "            execution_time = pd.Timestamp.now() - self._start_time\n",
    "            mean_execution_time = execution_time.total_seconds() / self._counter\n",
    "\n",
    "            sim_time = self._environment.current_simulation_timestamp.tz_convert(\n",
    "                    time_zone\n",
    "            )\n",
    "            percent_complete = int(\n",
    "                    100.0 * (self._counter / self._num_timesteps_in_episode)\n",
    "            )\n",
    "\n",
    "            if self._replay_buffer is not None:\n",
    "                rb_size = self._replay_buffer.num_frames()\n",
    "                rb_string = \" Replay Buffer Size: %d\" % rb_size\n",
    "            else:\n",
    "                rb_string = \"\"\n",
    "\n",
    "            print(\n",
    "                    \"Step %5d of %5d (%3d%%) Sim Time: %s Reward: %2.2f Cumulative\"\n",
    "                    \" Reward: %8.2f Execution Time: %s Mean Execution Time: %3.2fs %s\"\n",
    "                    % (\n",
    "                            self._environment._step_count,\n",
    "                            self._num_timesteps_in_episode,\n",
    "                            percent_complete,\n",
    "                            sim_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                            reward,\n",
    "                            self._cumulative_reward,\n",
    "                            execution_time,\n",
    "                            mean_execution_time,\n",
    "                            rb_string,\n",
    "                    )\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDgizVLzRti1"
   },
   "source": [
    "# Reinforcement Learning Control\n",
    "In the previous section we used a simple schedule to control the HVAC setpoints, however in this section, we configure and train a Reinforcement Learning (RL) agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Utilities to configure networks for the RL Agent.\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer='glorot_uniform',\n",
    ")\n",
    "\n",
    "\n",
    "def logging_info(*args):\n",
    "    logging.info(*args)\n",
    "    print(*args)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "    return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "    return tf.keras.layers.Lambda(lambda x: x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC Critic Network\n",
    "\n",
    "- obs network learns meaningful representation of state \n",
    "- action network learns meaningful representation of action\n",
    "- joint network $f(z_a, z_s) \\rightarrow \\hat{Q}(s, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_critic_network(obs_fc_layer_units, action_fc_layer_units, joint_fc_layer_units):\n",
    "    \"\"\"Create a sequential critic network.\"\"\"\n",
    "    # Split the inputs into observations and actions.\n",
    "    def split_inputs(inputs):\n",
    "        obs, action = inputs[0], inputs[1]\n",
    "        if len(obs.shape) == 1:\n",
    "            obs = tf.expand_dims(obs, 0)\n",
    "        if len(action.shape) == 1:\n",
    "            action = tf.expand_dims(action, 0)\n",
    "        return {'observation': obs, 'action': action}\n",
    "\n",
    "    # Create an observation network.\n",
    "    obs_network = (\n",
    "        create_fc_network(obs_fc_layer_units) if obs_fc_layer_units else create_identity_layer()\n",
    "    )\n",
    "\n",
    "    # Create an action network.\n",
    "    action_network = (\n",
    "        create_fc_network(action_fc_layer_units) if action_fc_layer_units else create_identity_layer()\n",
    "    )\n",
    "\n",
    "    # Create a joint network.\n",
    "    joint_network = (\n",
    "        create_fc_network(joint_fc_layer_units) if joint_fc_layer_units else create_identity_layer()\n",
    "    )\n",
    "\n",
    "    # Final layer.\n",
    "    value_layer = tf.keras.layers.Dense(1, kernel_initializer='glorot_uniform')\n",
    "\n",
    "    return sequential.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Lambda(split_inputs),\n",
    "            nest_map.NestMap({'observation': obs_network, 'action': action_network}),\n",
    "            nest_map.NestFlatten(),\n",
    "            tf.keras.layers.Concatenate(),\n",
    "            joint_network,\n",
    "            value_layer,\n",
    "            inner_reshape.InnerReshape(current_shape=[1], new_shape=[]),\n",
    "        ],\n",
    "        name='sequential_critic',\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just an action, returns prob distribution over actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "zBjFBpkabFHR"
   },
   "outputs": [],
   "source": [
    "class _TanhNormalProjectionNetworkWrapper(\n",
    "        tanh_normal_projection_network.TanhNormalProjectionNetwork\n",
    "):\n",
    "    \"\"\"Wrapper to pass predefined `outer_rank` to underlying projection net.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_spec, predefined_outer_rank=1):\n",
    "        super(_TanhNormalProjectionNetworkWrapper, self).__init__(sample_spec)\n",
    "        self.predefined_outer_rank = predefined_outer_rank\n",
    "\n",
    "    def call(self, inputs, network_state=(), **kwargs):\n",
    "        kwargs['outer_rank'] = self.predefined_outer_rank\n",
    "        if 'step_type' in kwargs:\n",
    "            del kwargs['step_type']\n",
    "        return super(_TanhNormalProjectionNetworkWrapper, self).call(inputs, **kwargs)\n",
    "\n",
    "\n",
    "def create_sequential_actor_network(actor_fc_layers, action_tensor_spec):\n",
    "    \"\"\"Create a sequential actor network.\"\"\"\n",
    "    def expand_dims(inputs):\n",
    "        if len(inputs.shape) == 1:\n",
    "            return tf.expand_dims(inputs, 0)\n",
    "        return inputs\n",
    "    \n",
    "    def tile_as_nest(non_nested_output):\n",
    "        return tf.nest.map_structure(\n",
    "                lambda _: non_nested_output, action_tensor_spec\n",
    "        )\n",
    "\n",
    "    return sequential.Sequential(\n",
    "            [tf.keras.layers.Lambda(expand_dims)]  \n",
    "            + [dense(num_units) for num_units in actor_fc_layers]\n",
    "            + [tf.keras.layers.Lambda(tile_as_nest)]\n",
    "            + [nest_map.NestMap(tf.nest.map_structure(_TanhNormalProjectionNetworkWrapper, \n",
    "                                                      action_tensor_spec))])\n",
    "            #+ [tf.keras.layers.Lambda(squeeze_output)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g6pE6v2bb8O"
   },
   "source": [
    "Set the configuration parameters for the SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "id": "CeVkerwYcng2"
   },
   "outputs": [],
   "source": [
    "# @title Set the RL Agent's parameters\n",
    "\n",
    "# Actor network fully connected layers.\n",
    "actor_fc_layers = (128, 128)\n",
    "# Critic network observation fully connected layers.\n",
    "critic_obs_fc_layers = (128, 64)\n",
    "# Critic network action fully connected layers.\n",
    "critic_action_fc_layers = (128, 64)\n",
    "# Critic network joint fully connected layers.\n",
    "critic_joint_fc_layers = (128, 64)\n",
    "\n",
    "batch_size = 256\n",
    "actor_learning_rate = 3e-4\n",
    "critic_learning_rate = 3e-4\n",
    "alpha_learning_rate = 3e-4\n",
    "gamma = 0.99\n",
    "target_update_tau= 0.005\n",
    "target_update_period= 1\n",
    "reward_scale_factor = 1.0\n",
    "\n",
    "# Replay params\n",
    "replay_capacity = 1000000\n",
    "debug_summaries = True\n",
    "summarize_grads_and_vars = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhTPXjtebMZD"
   },
   "source": [
    "## Initialize the SAC agent\n",
    "\n",
    "Of all the Reinforcement learning algorithms, we have chosen [Soft Actor Cirtic (SAC)](https://arxiv.org/abs/1801.01290) because its proven performance on evironments with  high-dimensional states and real-valued actions.\n",
    "\n",
    "In this notebook we illustrate the use of the buidling control environment using the SAC implementation in [TF-Agents](https://www.tensorflow.org/agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "id": "NW0pzLvjbSnP"
   },
   "outputs": [],
   "source": [
    "# @title Construct the SAC agent\n",
    "\n",
    "_, action_tensor_spec, time_step_tensor_spec = spec_utils.get_tensor_specs(\n",
    "    collect_env\n",
    ")\n",
    "\n",
    "actor_net = create_sequential_actor_network(\n",
    "    actor_fc_layers=actor_fc_layers, action_tensor_spec=action_tensor_spec\n",
    ")\n",
    "\n",
    "critic_net = create_sequential_critic_network(\n",
    "    obs_fc_layer_units=critic_obs_fc_layers,\n",
    "    action_fc_layer_units=critic_action_fc_layers,\n",
    "    joint_fc_layer_units=critic_joint_fc_layers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.13324027], dtype=float32)>,\n",
       " ())"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_input = tf.random.normal((53,))\n",
    "action_input = tf.random.normal((1, 2))\n",
    "\n",
    "critic_net((state_input, action_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.SquashToSpecNormal 'SquashToSpecNormal' batch_shape=[1] event_shape=? dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_net(state_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = train_utils.create_train_step()\n",
    "\n",
    "agent = sac_agent.SacAgent(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.keras.optimizers.Adam(learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=critic_learning_rate\n",
    "    ),\n",
    "    alpha_optimizer=tf.keras.optimizers.Adam(learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.math.squared_difference,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=None,\n",
    "    debug_summaries=debug_summaries,\n",
    "    summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "    train_step_counter=train_step,\n",
    ")\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5hNdgZBG5BZ"
   },
   "source": [
    "Below we construct a replay buffer using reverb. The replay buffer is populated with state-action-reward-state tuples during collect. Thie allows the agent to relive past experiences, and prevents the model from overfitting in the local neighborhood.\n",
    "\n",
    "During traning, the agent samples from the replay buffer. This helps decorrelate the training data in a way that randomization of a training set would in supervised learning. Otherwise, in most environments the experience in a window of time is highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "id": "vX2zGUWJGWAl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverb_checkpoint_dir=/burg/home/ssa2206/sbsim_dual_control/output/reverb_checkpoint\n",
      "reverb_server_port=36915\n",
      "num_frames in replay buffer=575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /burg/home/ssa2206/sbsim_dual_control/output/reverb_checkpoint.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /burg/home/ssa2206/sbsim_dual_control/output/reverb_checkpoint\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:286] Loading checkpoint from /burg/home/ssa2206/sbsim_dual_control/output/reverb_checkpoint/2024-12-15T17:44:15.352512536-05:00\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:295] Loading and verifying metadata of the checkpointed tables.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:341] Metadata for table 'uniform_table' was successfully loaded and verified.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:404] Successfully loaded and verified metadata for all (1) tables. We'll now proceed to read the data referenced by the items in the table.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 1 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 101 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 201 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 301 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 401 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 501 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 601 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 701 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 801 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 901 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 1001 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 1101 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:447] Still reading compressed trajectory data. 1201 records have been read so far.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:457] Completed reading compressed trajectory data. We'll now start assembling the checkpointed tables.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:534] Table uniform_table and 575 items have been successfully loaded from checkpoint at path /burg/home/ssa2206/sbsim_dual_control/output/reverb_checkpoint/2024-12-15T17:44:15.352512536-05:00.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:540] Successfully loaded 1 tables from /burg/home/ssa2206/sbsim_dual_control/output/reverb_checkpoint/2024-12-15T17:44:15.352512536-05:00\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 36915\n"
     ]
    }
   ],
   "source": [
    "# @title Set up the replay buffer\n",
    "replay_capacity = 50000\n",
    "table_name = 'uniform_table'\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    ")\n",
    "\n",
    "reverb_checkpoint_dir = output_data_path + \"/reverb_checkpoint\"\n",
    "reverb_port = None\n",
    "print('reverb_checkpoint_dir=%s' %reverb_checkpoint_dir)\n",
    "reverb_checkpointer = reverb.platform.checkpointers_lib.DefaultCheckpointer(path=reverb_checkpoint_dir)\n",
    "reverb_server = reverb.Server([table], port=reverb_port, checkpointer=reverb_checkpointer)\n",
    "logging_info('reverb_server_port=%d' % reverb_server.port)\n",
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    sequence_length=2,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server,\n",
    ")\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "    reverb_replay.py_client, table_name, sequence_length=2, stride_length=1\n",
    ")\n",
    "print('num_frames in replay buffer=%d' %reverb_replay.num_frames())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH7LQZ_Pd0vY"
   },
   "source": [
    "For simplicity, we'll grab eval and collact policies and give them short variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "BwY7StuMkuV4"
   },
   "outputs": [],
   "source": [
    "# @title Access the eval and collect policies\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6klSPQeGsPLz"
   },
   "source": [
    "In the next section we define observer classes that enable printing model and environment output as the scenario evolves to who you the percentage of the episode, the timestamp in the scenario, cumulative reward, and the execution time.\n",
    "\n",
    "We also provide a plot observer that periodically outputs the performance charts and the temperature gradient across both floors of the buidling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_render_plot_observer = RenderAndPlotObserver(\n",
    "    render_interval_steps=144, environment=initial_collect_env\n",
    ")\n",
    "initial_collect_print_status_observer = PrintStatusObserver(\n",
    "    status_interval_steps=2,\n",
    "    environment=initial_collect_env,\n",
    "    replay_buffer=reverb_replay,\n",
    ")\n",
    "collect_render_plot_observer = RenderAndPlotObserver(render_interval_steps=2, environment=collect_env)\n",
    "collect_print_status_observer = PrintStatusObserver(\n",
    "    status_interval_steps=1,\n",
    "    environment=collect_env,\n",
    "    replay_buffer=reverb_replay,\n",
    ")\n",
    "eval_render_plot_observer = RenderAndPlotObserver(\n",
    "    render_interval_steps=2, environment=eval_env\n",
    ")\n",
    "eval_print_status_observer = PrintStatusObserver(status_interval_steps=1, \n",
    "                    environment=eval_env, replay_buffer=reverb_replay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el732oZItQjO"
   },
   "source": [
    "In the following cell, we shall run the baseline control on the scenario to populate the replay buffer. We will use the schedule policy we build above to simulate training off-policy from recorded telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hod_cos_index = collect_env._field_names.index('hod_cos_000')\n",
    "hod_sin_index = collect_env._field_names.index('hod_sin_000')\n",
    "dow_cos_index = collect_env._field_names.index('dow_cos_000')\n",
    "dow_sin_index = collect_env._field_names.index('dow_sin_000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "id": "ZGq3SY0kKwsa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# @title Populate the replay buffer with data from baseline control\n",
    "\n",
    "def populate_replay_buffer():\n",
    "    ts = collect_env.reset()\n",
    "    local_start_time = collect_env.current_simulation_timestamp.tz_convert(tz = 'US/Pacific')\n",
    "\n",
    "    action_normalizers = collect_env._action_normalizers\n",
    "    from run_utils import SchedulePolicy\n",
    "    observation_spec, action_spec, time_step_spec = spec_utils.get_tensor_specs(collect_env)\n",
    "    hod_cos_index = collect_env._field_names.index('hod_cos_000')\n",
    "    hod_sin_index = collect_env._field_names.index('hod_sin_000')\n",
    "    dow_cos_index = collect_env._field_names.index('dow_cos_000')\n",
    "    dow_sin_index = collect_env._field_names.index('dow_sin_000')\n",
    "    schedule_policy = SchedulePolicy(\n",
    "        time_step_spec= time_step_spec,\n",
    "        action_spec= action_spec,\n",
    "        dow_sin_index=dow_sin_index,\n",
    "        dow_cos_index=dow_cos_index,\n",
    "        hod_sin_index=hod_sin_index,\n",
    "        hod_cos_index=hod_cos_index,\n",
    "        local_start_time=local_start_time,\n",
    "        action_normalizers=action_normalizers,\n",
    "\n",
    "    )\n",
    "    initial_collect_actor = actor.Actor(initial_collect_env, schedule_policy, train_step,\n",
    "                                        steps_per_run=initial_collect_env._num_timesteps_in_episode,\n",
    "                                        observers=[rb_observer, initial_collect_print_status_observer, \n",
    "                                                   initial_collect_render_plot_observer]\n",
    "                                       )\n",
    "    initial_collect_actor.run()\n",
    "    reverb_replay.py_client.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3ZzWxqIunCz"
   },
   "source": [
    "Next wrap the replay buffer into a TF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "id": "ba7bilizt_qW",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(Trajectory(\n",
       "{'action': TensorSpec(shape=(256, 2, 2), dtype=tf.float32, name=None),\n",
       " 'discount': TensorSpec(shape=(256, 2), dtype=tf.float32, name=None),\n",
       " 'next_step_type': TensorSpec(shape=(256, 2), dtype=tf.int32, name=None),\n",
       " 'observation': TensorSpec(shape=(256, 2, 53), dtype=tf.float32, name=None),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(256, 2), dtype=tf.float32, name=None),\n",
       " 'step_type': TensorSpec(shape=(256, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(256, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(256, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(256, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(256, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(256, 2), dtype=tf.int32, name=None)))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Make a TF Dataset\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = reverb_replay.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1045071) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1045071) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1045071) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1045071) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1045071) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1045071) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    }
   ],
   "source": [
    "experience_batch, info = next(iter(dataset))\n",
    "states, actions, rewards = experience_batch.observation, experience_batch.action, experience_batch.reward\n",
    "at, atp1 = actions[:,0,:], actions[:, 1, :]\n",
    "st, stp1 = states[:,0,:], states[:, 1, :]\n",
    "# all useless: experience_batch.next_step_type, .step_type, .discount, .next_step_type, .policy_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAdbomqlyqpz"
   },
   "source": [
    "Set the number of training steps in a training iteration. This is the number of collect steps between gradient updates.\n",
    "\n",
    "Here we set the number of training steps to the length of a full episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6iWtSC-FKHMW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_steps_per_treining_iteration = collect_env._num_timesteps_in_episode\n",
    "collect_steps_per_treining_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdKA4Jy4YfJM"
   },
   "source": [
    "Next, we will define a *collect actor* and an *eval actor* that wrap the policy and the environment, and can execute and collect metrics.\n",
    "\n",
    "The principal difference between the collect actor and the eval actor, is that the collect actor will choose actions by drawing off the actor network distribution, choosing actions that have a high probability over actions with lower probability. This stochastic property enables the agent explore bettwer actions and improve the policy.\n",
    "\n",
    "However, the eval actor always chooses the action associated with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YqfMl5FuQpf"
   },
   "source": [
    "Here, we extract the collect and evaluation policies for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.actor_policy.ActorPolicy at 0x155479cf3880>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "form",
    "id": "TzwSaxYkeTh5"
   },
   "outputs": [],
   "source": [
    "# @title Convert the policies into TF Eager Policies\n",
    "\n",
    "tf_collect_policy = agent.collect_policy\n",
    "collect_actor_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_collect_policy, use_tf_function=True)\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "collect_actor = actor.Actor(\n",
    "    collect_env,\n",
    "    collect_policy,\n",
    "    train_step,\n",
    "    steps_per_run=collect_steps_per_treining_iteration,\n",
    "    metrics=actor.collect_metrics(1),\n",
    "    summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
    "    summary_interval=1,\n",
    "    observers=[\n",
    "        rb_observer,\n",
    "        env_step_metric,\n",
    "        collect_print_status_observer,\n",
    "        collect_render_plot_observer,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import actor_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': TensorSpec(shape=(53,), dtype=tf.float32, name='observation'),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_tensor_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(2,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_tensor_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy = actor_policy.ActorPolicy(\n",
    "#        time_step_spec=time_step_tensor_spec,\n",
    "#        action_spec=action_tensor_spec,\n",
    "#        actor_network=actor_net,\n",
    "#        training=False,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#policy.action(collect_actor._time_step, collect_actor._policy_state)\n",
    "#actor_net(collect_actor._time_step.observation)\n",
    "#collect_policy.action(collect_actor._time_step, collect_actor._policy_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.collect_policy._action_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.train.actor.Actor at 0x155479c67df0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "form",
    "id": "LWsI9znlqLvh"
   },
   "outputs": [],
   "source": [
    "# @title Define a TF-Agents Actor for collect and eval\n",
    "\n",
    "tf_greedy_policy = greedy_policy.GreedyPolicy(agent.policy)\n",
    "eval_greedy_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "    tf_greedy_policy, use_tf_function=True\n",
    ")\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "    eval_env,\n",
    "    eval_greedy_policy,\n",
    "    train_step,\n",
    "    episodes_per_run=1,\n",
    "    metrics=actor.eval_metrics(1),\n",
    "    summary_dir=os.path.join(root_dir, 'eval'),\n",
    "    summary_interval=1,\n",
    "    observers=[rb_observer, eval_print_status_observer, eval_render_plot_observer],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtoqyo8Ypn0Q"
   },
   "source": [
    "We will set the interval of saving the policies and writing critic, actor, and alphs losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "xums9Kxkxylw"
   },
   "outputs": [],
   "source": [
    "policy_save_interval = 1 # Save the policy after every learning step.\n",
    "learner_summary_interval = 1 # Produce a summary of the critic, actor, and alpha losses after every gradient update step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al5HNoiwvYO-"
   },
   "source": [
    "In the following cell we will define the agent learner, a TF-Agents wrapper around the process that performs gradiant-based updates to the actor and critic networks in the agent.\n",
    "\n",
    "You should see a statememt that shows you where the policies will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "form",
    "id": "Ah4oS9HLwOid"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies will be saved to saved_model_dir: /burg/home/ssa2206/sbsim_dual_control/root/policies\n"
     ]
    }
   ],
   "source": [
    "# @title Define an Agent Learner\n",
    "\n",
    "experience_dataset_fn = lambda: dataset\n",
    "\n",
    "saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "print('Policies will be saved to saved_model_dir: %s' % saved_model_dir)\n",
    "\n",
    "learning_triggers = [\n",
    "      triggers.PolicySavedModelTrigger(\n",
    "          saved_model_dir,\n",
    "          agent,\n",
    "          train_step,\n",
    "          interval=policy_save_interval,\n",
    "          metadata_metrics={triggers.ENV_STEP_METADATA_KEY: env_step_metric},\n",
    "      ),\n",
    "      triggers.StepPerSecondLogTrigger(train_step, interval=10),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "      root_dir,\n",
    "      train_step,\n",
    "      agent,\n",
    "      experience_dataset_fn,\n",
    "      triggers=learning_triggers,\n",
    "      strategy=None,\n",
    "      summary_interval=learner_summary_interval,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.train.learner.Learner at 0x155479bcf550>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the World Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from world_model import LinearWorldModel, generate_rollouts, add_to_replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = eval_env.reset()\n",
    "initial_state, initial_reward, initial_discount = timestep.observation, timestep.reward, timestep.discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout_length = int(4096/16)\n",
    "rollout_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverb_replay.py_client.server_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverb_server.port=36915, reverb_replay.num_frames()=575 in replay buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/reverb_service_impl.cc:168] Stored checkpoint to /burg/home/ssa2206/sbsim_dual_control/output/reverb_checkpoint/2024-12-15T17:56:19.635344187-05:00\n"
     ]
    }
   ],
   "source": [
    "reverb_replay.py_client.checkpoint()\n",
    "print(f\"{reverb_server.port=}, {reverb_replay.num_frames()=} in replay buffer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_DN734lZAwE"
   },
   "source": [
    "Finally we're ready to execute the RL traiing loop with SAC!\n",
    "\n",
    "You can sepcify the total number of trainng iterations, and the number of gradient steps per iteration. With fewer steps, the model will train more slowly, but more steps may make the agent less stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "# Disable TensorFlow logging\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_env._num_timesteps_in_episode = 4\n",
    "eval_env._num_timesteps_in_episode = 4\n",
    "\n",
    "\n",
    "\n",
    "timestep = eval_env.reset()\n",
    "initial_state, initial_reward, initial_discount = timestep.observation, timestep.reward, timestep.discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_steps_per_treining_iteration = collect_env._num_timesteps_in_episode\n",
    "collect_steps_per_treining_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n"
     ]
    }
   ],
   "source": [
    "# @title Execute the training loop\n",
    "\n",
    "num_training_episodes = 1\n",
    "num_gradient_updates_per_training_iteration = 2\n",
    "world_model_training_steps = 4\n",
    "rollout_length = 8#int(4096/16)\n",
    "\n",
    "# Collect the performance results with the untrained model.\n",
    "#eval_actor.run_and_log()\n",
    "\n",
    "logging_info('Training.')\n",
    "\n",
    "# @title Modified Training Loop with World Model\n",
    "# Initialize world model\n",
    "state_dim = collect_env.observation_spec().shape[0]\n",
    "action_dim = collect_env.action_spec().shape[0]\n",
    "world_model = LinearWorldModel(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_world_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_world_model\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_world_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_world_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"show_images\" style=\"border-spacing:0px;\"><tr><td style=\"padding:1px;\"><div style=\"display:flex; align-items:left;\">\n",
       "      <div style=\"display:flex; flex-direction:column; align-items:center;\">\n",
       "      <div>Environment 2023-07-06 07:00:00+00:00</div><div><img width=\"1004\" height=\"744\" style=\"image-rendering:auto; object-fit:cover;\" src=\"data:image/png;base64,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\"/></div></div></div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     0 of   576 ( 70%) Sim Time: 2023-07-06 00:00 Reward: 0.00 Cumulative Reward:    -7.26 Execution Time: 0 days 00:14:00.549950 Mean Execution Time: 2.06s  Replay Buffer Size: 915\n"
     ]
    }
   ],
   "source": [
    "for iter in range(num_training_episodes):\n",
    "    print('Training iteration: ', iter)\n",
    "    print(\"\\nRunning Collect Actor\\n\")\n",
    "    #collect_actor.run() # Collect real experiences using agent.collect_policy\n",
    "    print(\"\\nTraining World Model and adding synthetic experiences\\n\")\n",
    "    train_world_model(world_model, reverb_replay, batch_size, training_steps=world_model_training_steps)\n",
    "    initial_state, initial_reward, initial_discount = ts.observation, ts.reward, ts.discount\n",
    "\n",
    "    synthetic_rollouts = generate_rollouts(world_model, initial_state, \n",
    "                                            initial_reward, initial_discount, collect_policy, rollout_length)\n",
    "    rollout_length *= 2\n",
    "    for exp in synthetic_rollouts:\n",
    "        add_to_replay_buffer(rb_observer, exp)\n",
    "\n",
    "    # Train agent with both real and synthetic data\n",
    "    print(\"\\nTraining Agent\\n\")\n",
    "    loss_info = agent_learner.run(iterations=num_gradient_updates_per_training_iteration)\n",
    "\n",
    "    logging_info(\n",
    "        'Actor Loss: %6.2f, Critic Loss: %6.2f, Alpha Loss: %6.2f '\n",
    "        % (\n",
    "            loss_info.extra.actor_loss.numpy(),\n",
    "            loss_info.extra.critic_loss.numpy(),\n",
    "            loss_info.extra.alpha_loss.numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    eval_actor.run_and_log()\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = collect_actor._driver.policy\n",
    "\n",
    "action_step = pol.action(collect_actor._time_step, collect_actor._policy_state)\n",
    "action_step.action\n",
    "self = collect_env\n",
    "\n",
    "#action = self._format_action(action_step.action, self._action_names)\n",
    "#timestamp = conversion_utils.pandas_to_proto_timestamp(\n",
    "#    self.building.current_timestamp\n",
    "#)\n",
    "#action_request = smart_control_building_pb2.ActionRequest(timestamp=timestamp)\n",
    "\n",
    "#action_array = action\n",
    "#action = {}\n",
    "#for i in range(len(self._action_names)):\n",
    "#    action[self._action_names[i]] = action_array[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Free Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# @title Execute the training loop\n",
    "def train_SAC_baseline():\n",
    "    num_training_iterations = 10\n",
    "    num_gradient_updates_per_training_iteration = 100\n",
    "\n",
    "    # Collect the performance results with teh untrained model.\n",
    "    eval_actor.run_and_log()\n",
    "\n",
    "    logging_info('Training.')\n",
    "\n",
    "    # log_dir = root_dir + '/train'\n",
    "    # with tf.summary.create_file_writer(log_dir).as_default() as writer:   \n",
    "    for iter in range(num_training_iterations):\n",
    "        print('Training iteration: ', iter)\n",
    "        # Let the collect actor run, using its stochastic action selection policy.\n",
    "        collect_actor.run()\n",
    "        logging_info(\n",
    "            'Executing %d gradient updates.'\n",
    "            %num_gradient_updates_per_training_iteration\n",
    "        )\n",
    "        # Now, with the additional collectsteps in the replay buffer,\n",
    "        # allow the agent to make additional policy improvements.\n",
    "        loss_info = agent_learner.run(\n",
    "            iterations=num_gradient_updates_per_training_iteration\n",
    "        )\n",
    "\n",
    "        # writer.flush()\n",
    "        logging_info(\n",
    "            'Actor Loss: %6.2f, Critic Loss: %6.2f, Alpha Loss: %6.2f '\n",
    "            % (\n",
    "                loss_info.extra.actor_loss.numpy(),\n",
    "                loss_info.extra.critic_loss.numpy(),\n",
    "                loss_info.extra.alpha_loss.numpy(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logging_info('Evaluating.')\n",
    "\n",
    "        _ = eval_env.reset()\n",
    "        # Run the eval actor after the training iteration, and get its performance.\n",
    "        eval_actor.run_and_log()\n",
    "        # Flush the buffer every `flush_frequency` iterations\n",
    "        if iter % flush_frequency == 0:\n",
    "            f.flush()\n",
    "\n",
    "    rb_observer.close()\n",
    "    reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect stdout to a file\n",
    "original_stdout = sys.stdout  # Save the original stdout\n",
    "save_to_file = False\n",
    "train_baseline = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(save_to_file and train_baseline):\n",
    "    print(\"Sending output to text file\")\n",
    "    with open('/burg/home/ssa2206/sbsim_dual_control/smart_control/notebooks/logs/SAC_training.txt', 'w') as f:\n",
    "        sys.stdout = f  # Redirect stdout to the file\n",
    "        try:\n",
    "            train_SAC_baseline()\n",
    "\n",
    "        finally:\n",
    "            sys.stdout = original_stdout  # Restore original stdout\n",
    "            print(\"Finished training run\")\n",
    "elif(train_baseline):\n",
    "    train_SAC_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "L7w-mjPcH7u6",
    "kTtVb9wbRsKU",
    "86IIF7FrfJ_2",
    "SDgizVLzRti1"
   ],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1a2nzc-VcaGRTpsEFj3FgqRZY0Lk1dgrW",
     "timestamp": 1705074752110
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
