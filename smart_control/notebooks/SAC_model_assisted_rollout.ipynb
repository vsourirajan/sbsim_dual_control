{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "nQnmcm0oI1Q-"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vekhJpsOxLK"
   },
   "source": [
    "#SBSim: A tutorial of using Reinforcement Learning for Optimizing Energy Use and Minimizing Carbon Emission in Office Buildings\n",
    "\n",
    "___\n",
    "\n",
    "Commercial office buildings contribute 17 percent of Carbon Emissions in the US, according to the US Energy Information Administration (EIA), and improving their efficiency will reduce their environmental burden and operating cost. A major contributor of energy consumption in these buildings are the Heating, Ventilation, and Air Conditioning (HVAC) devices. HVAC devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) agent is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many practical challenges. Most existing work on applying RL to this important task either makes use of proprietary data, or focuses on expensive and proprietary simulations that may not be grounded in the real world. We present the Smart Buildings Control Suite, the first open source interactive HVAC control dataset extracted from live sensor measurements of devices in real office buildings. The dataset consists of two components: real-world historical data from two buildings, for offline RL, and a lightweight interactive simulator for each of these buildings, calibrated using the historical data, for online and model-based RL. For ease of use, our RL environments are all compatible with the OpenAI gym environment standard. We believe this benchmark will accelerate progress and collaboration on HVAC optimization.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook accompanies the paper titled, **Real-World Data and Calibrated Simulation Suite for Offline Training of Reinforcement Learning Agents to Optimize Energy and Emission in Office Buildings** by Judah Goldfeder and John Sipple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7w-mjPcH7u6"
   },
   "source": [
    "#Smart Buildings Simulator Soft Actor Critic Demo\n",
    "\n",
    "This notebook runs through training a Soft Actor Critic agent on an HVAC building simulator that has been calibrated from real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "YchP7JXbSXS1"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "from dataclasses import dataclass\n",
    "import datetime, pytz\n",
    "import enum\n",
    "import functools\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import time\n",
    "from typing import Final, Sequence\n",
    "from typing import Optional\n",
    "from typing import Union, cast\n",
    "os.environ['WRAPT_DISABLE_EXTENSIONS'] = 'true'\n",
    "from absl import logging\n",
    "import gin\n",
    "import gin\n",
    "from matplotlib import patches\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import reverb\n",
    "import mediapy as media\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "sys.path.append('/home/vaibhav/sbsim_model_based')\n",
    "from smart_control.environment import environment\n",
    "from smart_control.proto import smart_control_building_pb2\n",
    "from smart_control.proto import smart_control_normalization_pb2\n",
    "from smart_control.reward import electricity_energy_cost\n",
    "from smart_control.reward import natural_gas_energy_cost\n",
    "from smart_control.reward import setpoint_energy_carbon_regret\n",
    "from smart_control.reward import setpoint_energy_carbon_reward\n",
    "from smart_control.simulator import randomized_arrival_departure_occupancy\n",
    "from smart_control.simulator import rejection_simulator_building\n",
    "from smart_control.simulator import simulator_building\n",
    "from smart_control.simulator import step_function_occupancy\n",
    "from smart_control.simulator import stochastic_convection_simulator\n",
    "from smart_control.utils import bounded_action_normalizer\n",
    "from smart_control.utils import building_renderer\n",
    "from smart_control.utils import controller_reader\n",
    "from smart_control.utils import controller_writer\n",
    "from smart_control.utils import conversion_utils\n",
    "from smart_control.utils import observation_normalizer\n",
    "from smart_control.utils import reader_lib\n",
    "from smart_control.utils import writer_lib\n",
    "from smart_control.utils import histogram_reducer\n",
    "from smart_control.utils import environment_utils\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.train import triggers\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory as trajectory_lib\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import transition\n",
    "from tf_agents.typing import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "sDDU5FmLkYo-"
   },
   "outputs": [],
   "source": [
    "# @title Set local runtime configurations\n",
    "\n",
    "\n",
    "def logging_info(*args):\n",
    "  logging.info(*args)\n",
    "  print(*args)\n",
    "\n",
    "data_path = \"/home/vaibhav/sbsim_model_based/smart_control/configs/resources/sb1/\" #@param {type:\"string\"}\n",
    "metrics_path = \"/home/vaibhav/sbsim_model_based/metrics\" #@param {type:\"string\"}\n",
    "output_data_path = '/home/vaibhav/sbsim_model_based/output' #@param {type:\"string\"}\n",
    "root_dir = \"/home/vaibhav/sbsim_model_based/root\" #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_histogram_reducer():\n",
    "\n",
    "\n",
    "    reader = controller_reader.ProtoReader(data_path)\n",
    "\n",
    "    hr = histogram_reducer.HistogramReducer(\n",
    "        histogram_parameters_tuples=histogram_parameters_tuples,\n",
    "        reader=reader,\n",
    "        normalize_reduce=True,\n",
    "        )\n",
    "    return hr\n",
    "\n",
    "!mkdir -p $root_dir\n",
    "!mkdir -p $output_data_path\n",
    "!mkdir -p $metrics_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remap_filepath(filepath) -> str:\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "JV_2oCn2uQU4"
   },
   "outputs": [],
   "source": [
    "# @title Plotting Utities\n",
    "reward_shift = 0\n",
    "reward_scale = 1.0\n",
    "person_productivity_hour = 300.0\n",
    "\n",
    "KELVIN_TO_CELSIUS = 273.15\n",
    "\n",
    "\n",
    "def render_env(env: environment.Environment):\n",
    "  \"\"\"Renders the environment.\"\"\"\n",
    "  building_layout = env.building._simulator._building._floor_plan\n",
    "\n",
    "  # create a renderer\n",
    "  renderer = building_renderer.BuildingRenderer(building_layout, 1)\n",
    "\n",
    "  # get the current temps to render\n",
    "  # this also is not ideal, since the temps are not fully exposed.\n",
    "  # V Ideally this should be a publicly accessable field\n",
    "  temps = env.building._simulator._building.temp\n",
    "\n",
    "  input_q = env.building._simulator._building.input_q\n",
    "\n",
    "  # render\n",
    "  vmin = 285\n",
    "  vmax = 305\n",
    "  image = renderer.render(\n",
    "      temps,\n",
    "      cmap='bwr',\n",
    "      vmin=vmin,\n",
    "      vmax=vmax,\n",
    "      colorbar=False,\n",
    "      input_q=input_q,\n",
    "      diff_range=0.5,\n",
    "      diff_size=1,\n",
    "  ).convert('RGB')\n",
    "  media.show_image(\n",
    "      image, title='Environment %s' % env.current_simulation_timestamp\n",
    "  )\n",
    "\n",
    "\n",
    "def get_energy_timeseries(reward_infos, time_zone: str) -> pd.DataFrame:\n",
    "  \"\"\"Returns a timeseries of energy rates.\"\"\"\n",
    "\n",
    "  start_times = []\n",
    "  end_times = []\n",
    "\n",
    "  device_ids = []\n",
    "  device_types = []\n",
    "  air_handler_blower_electrical_energy_rates = []\n",
    "  air_handler_air_conditioner_energy_rates = []\n",
    "  boiler_natural_gas_heating_energy_rates = []\n",
    "  boiler_pump_electrical_energy_rates = []\n",
    "\n",
    "  for reward_info in reward_infos:\n",
    "    end_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_info.end_timestamp\n",
    "    ).tz_convert(time_zone)\n",
    "    start_timestamp = end_timestamp - pd.Timedelta(300, unit='second')\n",
    "\n",
    "    for air_handler_id in reward_info.air_handler_reward_infos:\n",
    "      start_times.append(start_timestamp)\n",
    "      end_times.append(end_timestamp)\n",
    "\n",
    "      device_ids.append(air_handler_id)\n",
    "      device_types.append('air_handler')\n",
    "\n",
    "      air_handler_blower_electrical_energy_rates.append(\n",
    "          reward_info.air_handler_reward_infos[\n",
    "              air_handler_id\n",
    "          ].blower_electrical_energy_rate\n",
    "      )\n",
    "      air_handler_air_conditioner_energy_rates.append(\n",
    "          reward_info.air_handler_reward_infos[\n",
    "              air_handler_id\n",
    "          ].air_conditioning_electrical_energy_rate\n",
    "      )\n",
    "      boiler_natural_gas_heating_energy_rates.append(0)\n",
    "      boiler_pump_electrical_energy_rates.append(0)\n",
    "\n",
    "    for boiler_id in reward_info.boiler_reward_infos:\n",
    "      start_times.append(start_timestamp)\n",
    "      end_times.append(end_timestamp)\n",
    "\n",
    "      device_ids.append(boiler_id)\n",
    "      device_types.append('boiler')\n",
    "\n",
    "      air_handler_blower_electrical_energy_rates.append(0)\n",
    "      air_handler_air_conditioner_energy_rates.append(0)\n",
    "\n",
    "      boiler_natural_gas_heating_energy_rates.append(\n",
    "          reward_info.boiler_reward_infos[\n",
    "              boiler_id\n",
    "          ].natural_gas_heating_energy_rate\n",
    "      )\n",
    "      boiler_pump_electrical_energy_rates.append(\n",
    "          reward_info.boiler_reward_infos[boiler_id].pump_electrical_energy_rate\n",
    "      )\n",
    "\n",
    "  df_map = {\n",
    "      'start_time': start_times,\n",
    "      'end_time': end_times,\n",
    "      'device_id': device_ids,\n",
    "      'device_type': device_types,\n",
    "      'air_handler_blower_electrical_energy_rate': (\n",
    "          air_handler_blower_electrical_energy_rates\n",
    "      ),\n",
    "      'air_handler_air_conditioner_energy_rate': (\n",
    "          air_handler_air_conditioner_energy_rates\n",
    "      ),\n",
    "      'boiler_natural_gas_heating_energy_rate': (\n",
    "          boiler_natural_gas_heating_energy_rates\n",
    "      ),\n",
    "      'boiler_pump_electrical_energy_rate': boiler_pump_electrical_energy_rates,\n",
    "  }\n",
    "  df = pd.DataFrame(df_map).sort_values('start_time')\n",
    "  return df\n",
    "\n",
    "\n",
    "def get_outside_air_temperature_timeseries(\n",
    "    observation_responses,\n",
    "    time_zone: str,\n",
    ") -> pd.Series:\n",
    "  \"\"\"Returns a timeseries of outside air temperature.\"\"\"\n",
    "  temps = []\n",
    "  for i in range(len(observation_responses)):\n",
    "    temp = [\n",
    "        (\n",
    "            conversion_utils.proto_to_pandas_timestamp(\n",
    "                sor.timestamp\n",
    "            ).tz_convert(time_zone)\n",
    "            - pd.Timedelta(300, unit='second'),\n",
    "            sor.continuous_value,\n",
    "        )\n",
    "        for sor in observation_responses[i].single_observation_responses\n",
    "        if sor.single_observation_request.measurement_name\n",
    "        == 'outside_air_temperature_sensor'\n",
    "    ][0]\n",
    "    temps.append(temp)\n",
    "\n",
    "  res = list(zip(*temps))\n",
    "  return pd.Series(res[1], index=res[0]).sort_index()\n",
    "\n",
    "\n",
    "def get_reward_timeseries(\n",
    "    reward_infos,\n",
    "    reward_responses,\n",
    "    time_zone: str,\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Returns a timeseries of reward values.\"\"\"\n",
    "  cols = [\n",
    "      'agent_reward_value',\n",
    "      'electricity_energy_cost',\n",
    "      'carbon_emitted',\n",
    "      'occupancy',\n",
    "  ]\n",
    "  df = pd.DataFrame(columns=cols)\n",
    "\n",
    "  for i in range(min(len(reward_responses), len(reward_infos))):\n",
    "    step_start_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_infos[i].start_timestamp\n",
    "    ).tz_convert(time_zone)\n",
    "    step_end_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_infos[i].end_timestamp\n",
    "    ).tz_convert(time_zone)\n",
    "    delta_time_sec = (step_end_timestamp - step_start_timestamp).total_seconds()\n",
    "    occupancy = np.sum([\n",
    "        reward_infos[i].zone_reward_infos[zone_id].average_occupancy\n",
    "        for zone_id in reward_infos[i].zone_reward_infos\n",
    "    ])\n",
    "\n",
    "    df.loc[\n",
    "        conversion_utils.proto_to_pandas_timestamp(\n",
    "            reward_infos[i].start_timestamp\n",
    "        ).tz_convert(time_zone)\n",
    "    ] = [\n",
    "        reward_responses[i].agent_reward_value,\n",
    "        reward_responses[i].electricity_energy_cost,\n",
    "        reward_responses[i].carbon_emitted,\n",
    "        occupancy,\n",
    "    ]\n",
    "\n",
    "  df = df.sort_index()\n",
    "  df['cumulative_reward'] = df['agent_reward_value'].cumsum()\n",
    "  logging_info('Cumulative reward: %4.2f' % df.iloc[-1]['cumulative_reward'])\n",
    "  return df\n",
    "\n",
    "\n",
    "def format_plot(\n",
    "    ax1, xlabel: str, start_time: int, end_time: int, time_zone: str\n",
    "):\n",
    "  \"\"\"Formats a plot with common attributes.\"\"\"\n",
    "  ax1.set_facecolor('black')\n",
    "  ax1.xaxis.tick_top()\n",
    "  ax1.tick_params(axis='x', labelsize=12)\n",
    "  ax1.tick_params(axis='y', labelsize=12)\n",
    "  ax1.xaxis.set_major_formatter(\n",
    "      mdates.DateFormatter('%a %m/%d %H:%M', tz=pytz.timezone(time_zone))\n",
    "  )\n",
    "  ax1.grid(color='gray', linestyle='-', linewidth=1.0)\n",
    "  ax1.set_ylabel(xlabel, color='blue', fontsize=12)\n",
    "  ax1.set_xlim(left=start_time, right=end_time)\n",
    "  ax1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "  ax1.legend(prop={'size': 10})\n",
    "\n",
    "\n",
    "def plot_occupancy_timeline(\n",
    "    ax1, reward_timeseries: pd.DataFrame, time_zone: str\n",
    "):\n",
    "  local_times = [ts.tz_convert(time_zone) for ts in reward_timeseries.index]\n",
    "  ax1.plot(\n",
    "      local_times,\n",
    "      reward_timeseries['occupancy'],\n",
    "      color='cyan',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=2,\n",
    "      linestyle='-',\n",
    "      label='Num Occupants',\n",
    "  )\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Occupancy',\n",
    "      reward_timeseries.index.min(),\n",
    "      reward_timeseries.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_energy_cost_timeline(\n",
    "    ax1,\n",
    "    reward_timeseries: pd.DataFrame,\n",
    "    time_zone: str,\n",
    "    cumulative: bool = False,\n",
    "):\n",
    "  local_times = [ts.tz_convert(time_zone) for ts in reward_timeseries.index]\n",
    "  if cumulative:\n",
    "    feature_timeseries_cost = reward_timeseries[\n",
    "        'electricity_energy_cost'\n",
    "    ].cumsum()\n",
    "  else:\n",
    "    feature_timeseries_cost = reward_timeseries['electricity_energy_cost']\n",
    "  ax1.plot(\n",
    "      local_times,\n",
    "      feature_timeseries_cost,\n",
    "      color='magenta',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=2,\n",
    "      linestyle='-',\n",
    "      label='Electricity',\n",
    "  )\n",
    "\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Energy Cost [$]',\n",
    "      reward_timeseries.index.min(),\n",
    "      reward_timeseries.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_reward_timeline(ax1, reward_timeseries, time_zone):\n",
    "\n",
    "  local_times = [ts.tz_convert(time_zone) for ts in reward_timeseries.index]\n",
    "\n",
    "  ax1.plot(\n",
    "      local_times,\n",
    "      reward_timeseries['cumulative_reward'],\n",
    "      color='royalblue',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=6,\n",
    "      linestyle='-',\n",
    "      label='reward',\n",
    "  )\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Agent Reward',\n",
    "      reward_timeseries.index.min(),\n",
    "      reward_timeseries.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_energy_timeline(ax1, energy_timeseries, time_zone, cumulative=False):\n",
    "\n",
    "  def _to_kwh(\n",
    "      energy_rate: float,\n",
    "      step_interval: pd.Timedelta = pd.Timedelta(5, unit='minute'),\n",
    "  ) -> float:\n",
    "    kw_power = energy_rate / 1000.0\n",
    "    hwh_power = kw_power * step_interval / pd.Timedelta(1, unit='hour')\n",
    "    return hwh_power.cumsum()\n",
    "\n",
    "  timeseries = energy_timeseries[\n",
    "      energy_timeseries['device_type'] == 'air_handler'\n",
    "  ]\n",
    "\n",
    "  if cumulative:\n",
    "    feature_timeseries_ac = _to_kwh(\n",
    "        timeseries['air_handler_air_conditioner_energy_rate']\n",
    "    )\n",
    "    feature_timeseries_blower = _to_kwh(\n",
    "        timeseries['air_handler_blower_electrical_energy_rate']\n",
    "    )\n",
    "  else:\n",
    "    feature_timeseries_ac = (\n",
    "        timeseries['air_handler_air_conditioner_energy_rate'] / 1000.0\n",
    "    )\n",
    "    feature_timeseries_blower = (\n",
    "        timeseries['air_handler_blower_electrical_energy_rate'] / 1000.0\n",
    "    )\n",
    "\n",
    "  ax1.plot(\n",
    "      timeseries['start_time'],\n",
    "      feature_timeseries_ac,\n",
    "      color='magenta',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='-',\n",
    "      label='AHU Electricity',\n",
    "  )\n",
    "  ax1.plot(\n",
    "      timeseries['start_time'],\n",
    "      feature_timeseries_blower,\n",
    "      color='magenta',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='--',\n",
    "      label='FAN Electricity',\n",
    "  )\n",
    "\n",
    "  timeseries = energy_timeseries[energy_timeseries['device_type'] == 'boiler']\n",
    "  if cumulative:\n",
    "    feature_timeseries_gas = _to_kwh(\n",
    "        timeseries['boiler_natural_gas_heating_energy_rate']\n",
    "    )\n",
    "    feature_timeseries_pump = _to_kwh(\n",
    "        timeseries['boiler_pump_electrical_energy_rate']\n",
    "    )\n",
    "  else:\n",
    "    feature_timeseries_gas = (\n",
    "        timeseries['boiler_natural_gas_heating_energy_rate'] / 1000.0\n",
    "    )\n",
    "    feature_timeseries_pump = (\n",
    "        timeseries['boiler_pump_electrical_energy_rate'] / 1000.0\n",
    "    )\n",
    "\n",
    "  ax1.plot(\n",
    "      timeseries['start_time'],\n",
    "      feature_timeseries_gas,\n",
    "      color='lime',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='-',\n",
    "      label='BLR Gas',\n",
    "  )\n",
    "  ax1.plot(\n",
    "      timeseries['start_time'],\n",
    "      feature_timeseries_pump,\n",
    "      color='lime',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='--',\n",
    "      label='Pump Electricity',\n",
    "  )\n",
    "\n",
    "  if cumulative:\n",
    "    label = 'HVAC Energy Consumption [kWh]'\n",
    "  else:\n",
    "    label = 'HVAC Power Consumption [kW]'\n",
    "\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      label,\n",
    "      timeseries['start_time'].min(),\n",
    "      timeseries['end_time'].max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_carbon_timeline(ax1, reward_timeseries, time_zone, cumulative=False):\n",
    "  \"\"\"Plots carbon-emission timeline.\"\"\"\n",
    "\n",
    "  if cumulative:\n",
    "    feature_timeseries_carbon = reward_timeseries['carbon_emitted'].cumsum()\n",
    "  else:\n",
    "    feature_timeseries_carbon = reward_timeseries['carbon_emitted']\n",
    "  ax1.plot(\n",
    "      reward_timeseries.index,\n",
    "      feature_timeseries_carbon,\n",
    "      color='white',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='-',\n",
    "      label='Carbon',\n",
    "  )\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Carbon emission [kg]',\n",
    "      reward_timeseries.index.min(),\n",
    "      reward_timeseries.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def get_zone_timeseries(reward_infos, time_zone):\n",
    "  \"\"\"Converts reward infos to a timeseries dataframe.\"\"\"\n",
    "\n",
    "  start_times = []\n",
    "  end_times = []\n",
    "  zones = []\n",
    "  heating_setpoints = []\n",
    "  cooling_setpoints = []\n",
    "  zone_air_temperatures = []\n",
    "  air_flow_rate_setpoints = []\n",
    "  air_flow_rates = []\n",
    "  average_occupancies = []\n",
    "\n",
    "  for reward_info in reward_infos:\n",
    "    start_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_info.end_timestamp\n",
    "    ).tz_convert(time_zone) - pd.Timedelta(300, unit='second')\n",
    "    end_timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        reward_info.end_timestamp\n",
    "    ).tz_convert(time_zone)\n",
    "\n",
    "    for zone_id in reward_info.zone_reward_infos:\n",
    "      zones.append(zone_id)\n",
    "      start_times.append(start_timestamp)\n",
    "      end_times.append(end_timestamp)\n",
    "\n",
    "      heating_setpoints.append(\n",
    "          reward_info.zone_reward_infos[zone_id].heating_setpoint_temperature\n",
    "      )\n",
    "      cooling_setpoints.append(\n",
    "          reward_info.zone_reward_infos[zone_id].cooling_setpoint_temperature\n",
    "      )\n",
    "\n",
    "      zone_air_temperatures.append(\n",
    "          reward_info.zone_reward_infos[zone_id].zone_air_temperature\n",
    "      )\n",
    "      air_flow_rate_setpoints.append(\n",
    "          reward_info.zone_reward_infos[zone_id].air_flow_rate_setpoint\n",
    "      )\n",
    "      air_flow_rates.append(\n",
    "          reward_info.zone_reward_infos[zone_id].air_flow_rate\n",
    "      )\n",
    "      average_occupancies.append(\n",
    "          reward_info.zone_reward_infos[zone_id].average_occupancy\n",
    "      )\n",
    "\n",
    "  df_map = {\n",
    "      'start_time': start_times,\n",
    "      'end_time': end_times,\n",
    "      'zone': zones,\n",
    "      'heating_setpoint_temperature': heating_setpoints,\n",
    "      'cooling_setpoint_temperature': cooling_setpoints,\n",
    "      'zone_air_temperature': zone_air_temperatures,\n",
    "      'air_flow_rate_setpoint': air_flow_rate_setpoints,\n",
    "      'air_flow_rate': air_flow_rates,\n",
    "      'average_occupancy': average_occupancies,\n",
    "  }\n",
    "  return pd.DataFrame(df_map).sort_values('start_time')\n",
    "\n",
    "\n",
    "def get_action_timeseries(action_responses):\n",
    "  \"\"\"Converts action responses to a dataframe.\"\"\"\n",
    "  timestamps = []\n",
    "  device_ids = []\n",
    "  setpoint_names = []\n",
    "  setpoint_values = []\n",
    "  response_types = []\n",
    "  for action_response in action_responses:\n",
    "\n",
    "    timestamp = conversion_utils.proto_to_pandas_timestamp(\n",
    "        action_response.timestamp\n",
    "    )\n",
    "    for single_action_response in action_response.single_action_responses:\n",
    "      device_id = single_action_response.request.device_id\n",
    "      setpoint_name = single_action_response.request.setpoint_name\n",
    "      setpoint_value = single_action_response.request.continuous_value\n",
    "      response_type = single_action_response.response_type\n",
    "\n",
    "      timestamps.append(timestamp)\n",
    "      device_ids.append(device_id)\n",
    "      setpoint_names.append(setpoint_name)\n",
    "      setpoint_values.append(setpoint_value)\n",
    "      response_types.append(response_type)\n",
    "\n",
    "  return pd.DataFrame({\n",
    "      'timestamp': timestamps,\n",
    "      'device_id': device_ids,\n",
    "      'setpoint_name': setpoint_names,\n",
    "      'setpoint_value': setpoint_values,\n",
    "      'response_type': response_types,\n",
    "  })\n",
    "\n",
    "\n",
    "def plot_action_timeline(ax1, action_timeseries, action_tuple, time_zone):\n",
    "  \"\"\"Plots action timeline.\"\"\"\n",
    "\n",
    "  single_action_timeseries = action_timeseries[\n",
    "      (action_timeseries['device_id'] == action_tuple[0])\n",
    "      & (action_timeseries['setpoint_name'] == action_tuple[1])\n",
    "  ]\n",
    "  single_action_timeseries = single_action_timeseries.sort_values(\n",
    "      by='timestamp'\n",
    "  )\n",
    "\n",
    "  if action_tuple[1] in [\n",
    "      'supply_water_setpoint',\n",
    "      'supply_air_heating_temperature_setpoint',\n",
    "  ]:\n",
    "    single_action_timeseries['setpoint_value'] = (\n",
    "        single_action_timeseries['setpoint_value'] - KELVIN_TO_CELSIUS\n",
    "    )\n",
    "\n",
    "  ax1.plot(\n",
    "      single_action_timeseries['timestamp'],\n",
    "      single_action_timeseries['setpoint_value'],\n",
    "      color='lime',\n",
    "      marker=None,\n",
    "      alpha=1,\n",
    "      lw=4,\n",
    "      linestyle='-',\n",
    "      label=action_tuple[1],\n",
    "  )\n",
    "  title = '%s %s' % (action_tuple[0], action_tuple[1])\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Action',\n",
    "      single_action_timeseries['timestamp'].min(),\n",
    "      single_action_timeseries['timestamp'].max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def get_outside_air_temperature_timeseries(observation_responses, time_zone):\n",
    "  temps = []\n",
    "  for i in range(len(observation_responses)):\n",
    "    temp = [\n",
    "        (\n",
    "            conversion_utils.proto_to_pandas_timestamp(\n",
    "                sor.timestamp\n",
    "            ).tz_convert(time_zone),\n",
    "            sor.continuous_value,\n",
    "        )\n",
    "        for sor in observation_responses[i].single_observation_responses\n",
    "        if sor.single_observation_request.measurement_name\n",
    "        == 'outside_air_temperature_sensor'\n",
    "    ][0]\n",
    "    temps.append(temp)\n",
    "\n",
    "  res = list(zip(*temps))\n",
    "  return pd.Series(res[1], index=res[0]).sort_index()\n",
    "\n",
    "\n",
    "def plot_temperature_timeline(\n",
    "    ax1, zone_timeseries, outside_air_temperature_timeseries, time_zone\n",
    "):\n",
    "  zone_temps = pd.pivot_table(\n",
    "      zone_timeseries,\n",
    "      index=zone_timeseries['start_time'],\n",
    "      columns='zone',\n",
    "      values='zone_air_temperature',\n",
    "  ).sort_index()\n",
    "  zone_temps.quantile(q=0.25, axis=1)\n",
    "  zone_temp_stats = pd.DataFrame({\n",
    "      'min_temp': zone_temps.min(axis=1),\n",
    "      'q25_temp': zone_temps.quantile(q=0.25, axis=1),\n",
    "      'median_temp': zone_temps.median(axis=1),\n",
    "      'q75_temp': zone_temps.quantile(q=0.75, axis=1),\n",
    "      'max_temp': zone_temps.max(axis=1),\n",
    "  })\n",
    "\n",
    "  zone_heating_setpoints = (\n",
    "      pd.pivot_table(\n",
    "          zone_timeseries,\n",
    "          index=zone_timeseries['start_time'],\n",
    "          columns='zone',\n",
    "          values='heating_setpoint_temperature',\n",
    "      )\n",
    "      .sort_index()\n",
    "      .min(axis=1)\n",
    "  )\n",
    "  zone_cooling_setpoints = (\n",
    "      pd.pivot_table(\n",
    "          zone_timeseries,\n",
    "          index=zone_timeseries['start_time'],\n",
    "          columns='zone',\n",
    "          values='cooling_setpoint_temperature',\n",
    "      )\n",
    "      .sort_index()\n",
    "      .max(axis=1)\n",
    "  )\n",
    "\n",
    "  ax1.plot(\n",
    "      zone_cooling_setpoints.index,\n",
    "      zone_cooling_setpoints - KELVIN_TO_CELSIUS,\n",
    "      color='yellow',\n",
    "      lw=1,\n",
    "  )\n",
    "  ax1.plot(\n",
    "      zone_cooling_setpoints.index,\n",
    "      zone_heating_setpoints - KELVIN_TO_CELSIUS,\n",
    "      color='yellow',\n",
    "      lw=1,\n",
    "  )\n",
    "\n",
    "  ax1.fill_between(\n",
    "      zone_temp_stats.index,\n",
    "      zone_temp_stats['min_temp'] - KELVIN_TO_CELSIUS,\n",
    "      zone_temp_stats['max_temp'] - KELVIN_TO_CELSIUS,\n",
    "      facecolor='green',\n",
    "      alpha=0.8,\n",
    "  )\n",
    "  ax1.fill_between(\n",
    "      zone_temp_stats.index,\n",
    "      zone_temp_stats['q25_temp'] - KELVIN_TO_CELSIUS,\n",
    "      zone_temp_stats['q75_temp'] - KELVIN_TO_CELSIUS,\n",
    "      facecolor='green',\n",
    "      alpha=0.8,\n",
    "  )\n",
    "  ax1.plot(\n",
    "      zone_temp_stats.index,\n",
    "      zone_temp_stats['median_temp'] - KELVIN_TO_CELSIUS,\n",
    "      color='white',\n",
    "      lw=3,\n",
    "      alpha=1.0,\n",
    "  )\n",
    "  ax1.plot(\n",
    "      outside_air_temperature_timeseries.index,\n",
    "      outside_air_temperature_timeseries - KELVIN_TO_CELSIUS,\n",
    "      color='magenta',\n",
    "      lw=3,\n",
    "      alpha=1.0,\n",
    "  )\n",
    "  format_plot(\n",
    "      ax1,\n",
    "      'Temperature [C]',\n",
    "      zone_temp_stats.index.min(),\n",
    "      zone_temp_stats.index.max(),\n",
    "      time_zone,\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_timeseries_charts(reader, time_zone):\n",
    "  \"\"\"Plots timeseries charts.\"\"\"\n",
    "\n",
    "  observation_responses = reader.read_observation_responses(\n",
    "      pd.Timestamp.min, pd.Timestamp.max\n",
    "  )\n",
    "  action_responses = reader.read_action_responses(\n",
    "      pd.Timestamp.min, pd.Timestamp.max\n",
    "  )\n",
    "  reward_infos = reader.read_reward_infos(pd.Timestamp.min, pd.Timestamp.max)\n",
    "  reward_responses = reader.read_reward_responses(\n",
    "      pd.Timestamp.min, pd.Timestamp.max\n",
    "  )\n",
    "\n",
    "  if len(reward_infos) == 0 or len(reward_responses) == 0:\n",
    "    return\n",
    "\n",
    "  action_timeseries = get_action_timeseries(action_responses)\n",
    "  action_tuples = list(\n",
    "      set([\n",
    "          (row['device_id'], row['setpoint_name'])\n",
    "          for _, row in action_timeseries.iterrows()\n",
    "      ])\n",
    "  )\n",
    "\n",
    "  reward_timeseries = get_reward_timeseries(\n",
    "      reward_infos, reward_responses, time_zone\n",
    "  ).sort_index()\n",
    "  outside_air_temperature_timeseries = get_outside_air_temperature_timeseries(\n",
    "      observation_responses, time_zone\n",
    "  )\n",
    "  zone_timeseries = get_zone_timeseries(reward_infos, time_zone)\n",
    "  fig, axes = plt.subplots(\n",
    "      nrows=6 + len(action_tuples),\n",
    "      ncols=1,\n",
    "      gridspec_kw={\n",
    "          'height_ratios': [1, 1, 1, 1, 1, 1] + [1] * len(action_tuples)\n",
    "      },\n",
    "      squeeze=True,\n",
    "  )\n",
    "  fig.set_size_inches(24, 25)\n",
    "\n",
    "  energy_timeseries = get_energy_timeseries(reward_infos, time_zone)\n",
    "  plot_reward_timeline(axes[0], reward_timeseries, time_zone)\n",
    "  plot_energy_timeline(axes[1], energy_timeseries, time_zone, cumulative=True)\n",
    "  plot_energy_cost_timeline(\n",
    "      axes[2], reward_timeseries, time_zone, cumulative=True\n",
    "  )\n",
    "  plot_carbon_timeline(axes[3], reward_timeseries, time_zone, cumulative=True)\n",
    "  plot_occupancy_timeline(axes[4], reward_timeseries, time_zone)\n",
    "  plot_temperature_timeline(\n",
    "      axes[5], zone_timeseries, outside_air_temperature_timeseries, time_zone\n",
    "  )\n",
    "\n",
    "  for i, action_tuple in enumerate(action_tuples):\n",
    "    plot_action_timeline(\n",
    "        axes[6 + i], action_timeseries, action_tuple, time_zone\n",
    "    )\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTtVb9wbRsKU"
   },
   "source": [
    "# Load up the environment\n",
    "\n",
    "In this section we load up the Smart Buildings simulator environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "2fcYS1VBrvia"
   },
   "outputs": [],
   "source": [
    "# @title Utils for importing the environment.\n",
    "\n",
    "def load_environment(gin_config_file: str):\n",
    "  \"\"\"Returns an Environment from a config file.\"\"\"\n",
    "  # Global definition is required by Gin library to instantiate Environment.\n",
    "  global environment  # pylint: disable=global-variable-not-assigned\n",
    "  with gin.unlock_config():\n",
    "    gin.parse_config_file(gin_config_file)\n",
    "    return environment.Environment()  # pylint: disable=no-value-for-parameter\n",
    "\n",
    "\n",
    "def get_latest_episode_reader(\n",
    "    metrics_path: str,\n",
    ") -> controller_reader.ProtoReader:\n",
    "\n",
    "  episode_infos = controller_reader.get_episode_data(metrics_path).sort_index()\n",
    "  selected_episode = episode_infos.index[-1]\n",
    "  episode_path = os.path.join(metrics_path, selected_episode)\n",
    "  reader = controller_reader.ProtoReader(episode_path)\n",
    "  return reader\n",
    "\n",
    "@gin.configurable\n",
    "def get_histogram_path():\n",
    "  return data_path\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_reset_temp_values():\n",
    "  reset_temps_filepath = remap_filepath(\n",
    "      os.path.join(data_path, \"reset_temps.npy\")\n",
    "  )\n",
    "\n",
    "  return np.load(reset_temps_filepath)\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_zone_path():\n",
    "  return remap_filepath(\n",
    "      os.path.join(data_path, \"double_resolution_zone_1_2.npy\")\n",
    "  )\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_metrics_path():\n",
    "  return os.path.join(metrics_path, \"metrics\")\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def get_weather_path():\n",
    "  return remap_filepath(\n",
    "      os.path.join(\n",
    "          data_path, \"local_weather_moffett_field_20230701_20231122.csv\"\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10THzl_rSgFW"
   },
   "source": [
    "In the cell below, we will load the collect and eval environments. While we are loading the same environment, below, it would be useful to load the same building over near, but non-overlapping time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "XFeGO2TLRS1o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/sbsim_model_based/smart_control/configs/resources/sb1/sim_config.gin\n",
      "/home/vaibhav/sbsim_model_based/smart_control/configs/resources/sb1/sim_config.gin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/sbsim_model_based/smart_control/simulator/building_utils.py:283: UserWarning: Connected components is showing that there are 4 or fewer\n",
      "     rooms in your building. You may have your 0's and 1's inverted in the\n",
      "     floor_plan. Remember that for the connectedComponents function,\n",
      "     0's must code for exterior space and exterior or interior walls,\n",
      "     and 1's must code for interior space.\n",
      "  warnings.warn(\"\"\"Connected components is showing that there are 4 or fewer\n",
      "2024-12-02 15:25:02.131374: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.147750: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.147834: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.149145: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.149209: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.149251: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.195294: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.195375: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.195435: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-02 15:25:02.195475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21993 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:147: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(carbon_emission_rates) / 1.0e6 / 3600.0\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:152: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekday_energy_prices)\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:159: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekend_energy_prices)\n",
      "/home/vaibhav/sbsim_model_based/smart_control/simulator/building_utils.py:283: UserWarning: Connected components is showing that there are 4 or fewer\n",
      "     rooms in your building. You may have your 0's and 1's inverted in the\n",
      "     floor_plan. Remember that for the connectedComponents function,\n",
      "     0's must code for exterior space and exterior or interior walls,\n",
      "     and 1's must code for interior space.\n",
      "  warnings.warn(\"\"\"Connected components is showing that there are 4 or fewer\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:147: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(carbon_emission_rates) / 1.0e6 / 3600.0\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:152: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekday_energy_prices)\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:159: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekend_energy_prices)\n",
      "/home/vaibhav/sbsim_model_based/smart_control/simulator/building_utils.py:283: UserWarning: Connected components is showing that there are 4 or fewer\n",
      "     rooms in your building. You may have your 0's and 1's inverted in the\n",
      "     floor_plan. Remember that for the connectedComponents function,\n",
      "     0's must code for exterior space and exterior or interior walls,\n",
      "     and 1's must code for interior space.\n",
      "  warnings.warn(\"\"\"Connected components is showing that there are 4 or fewer\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:147: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(carbon_emission_rates) / 1.0e6 / 3600.0\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:152: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekday_energy_prices)\n",
      "/home/vaibhav/sbsim_model_based/smart_control/reward/electricity_energy_cost.py:159: UnitStrippedWarning: The unit of the quantity is stripped when downcasting to ndarray.\n",
      "  np.array(weekend_energy_prices)\n"
     ]
    }
   ],
   "source": [
    "# @gin.configurable\n",
    "# def to_timestamp(date_str: str) -> pd.Timestamp:\n",
    "#   \"\"\"Utilty macro for gin config.\"\"\"\n",
    "#   return pd.Timestamp(date_str)\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "# def local_time(time_str: str) -> pd.Timedelta:\n",
    "#   \"\"\"Utilty macro for gin config.\"\"\"\n",
    "#   return pd.Timedelta(time_str)\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "# def enumerate_zones(\n",
    "#     n_building_x: int, n_building_y: int\n",
    "# ) -> Sequence[tuple[int, int]]:\n",
    "#   \"\"\"Utilty macro for gin config.\"\"\"\n",
    "#   zone_coordinates = []\n",
    "#   for x in range(n_building_x):\n",
    "#     for y in range(n_building_y):\n",
    "#       zone_coordinates.append((x, y))\n",
    "#   return zone_coordinates\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "# def set_observation_normalization_constants(\n",
    "#     field_id: str, sample_mean: float, sample_variance: float\n",
    "# ) -> smart_control_normalization_pb2.ContinuousVariableInfo:\n",
    "#   return smart_control_normalization_pb2.ContinuousVariableInfo(\n",
    "#       id=field_id, sample_mean=sample_mean, sample_variance=sample_variance\n",
    "#   )\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "# def set_action_normalization_constants(\n",
    "#     min_native_value,\n",
    "#     max_native_value,\n",
    "#     min_normalized_value,\n",
    "#     max_normalized_value,\n",
    "# ) -> bounded_action_normalizer.BoundedActionNormalizer:\n",
    "#   return bounded_action_normalizer.BoundedActionNormalizer(\n",
    "#       min_native_value,\n",
    "#       max_native_value,\n",
    "#       min_normalized_value,\n",
    "#       max_normalized_value,\n",
    "#   )\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "# def get_zones_from_config(\n",
    "#     configuration_path: str,\n",
    "# ) -> Sequence[smart_control_building_pb2.ZoneInfo]:\n",
    "#   \"\"\"Loads up the zones as a gin macro.\"\"\"\n",
    "#   with gin.unlock_config():\n",
    "#     reader = reader_lib_google.RecordIoReader(input_dir=configuration_path)\n",
    "#     zone_infos = reader.read_zone_infos()\n",
    "#     return zone_infos\n",
    "\n",
    "\n",
    "# @gin.configurable\n",
    "# def get_devices_from_config(\n",
    "#     configuration_path: str,\n",
    "# ) -> Sequence[smart_control_building_pb2.DeviceInfo]:\n",
    "#   \"\"\"Loads up HVAC devices as a gin macro.\"\"\"\n",
    "#   with gin.unlock_config():\n",
    "#     reader = reader_lib_google.RecordIoReader(input_dir=configuration_path)\n",
    "#     device_infos = reader.read_device_infos()\n",
    "#     return device_infos\n",
    "\n",
    "# @title Load the environments\n",
    "\n",
    "histogram_parameters_tuples = (\n",
    "        ('zone_air_temperature_sensor',(285., 286., 287., 288, 289., 290., 291., 292., 293., 294., 295., 296., 297., 298., 299., 300.,301,302,303)),\n",
    "        ('supply_air_damper_percentage_command',(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)),\n",
    "        ('supply_air_flowrate_setpoint',( 0., 0.05, .1, .2, .3, .4, .5,  .7,  .9)),\n",
    "    )\n",
    "\n",
    "time_zone = 'US/Pacific'\n",
    "collect_scenario_config = os.path.join(data_path, \"sim_config.gin\")\n",
    "print(collect_scenario_config)\n",
    "eval_scenario_config = os.path.join(data_path, \"sim_config.gin\")\n",
    "print(eval_scenario_config)\n",
    "\n",
    "collect_env = load_environment(collect_scenario_config)\n",
    "\n",
    "# For efficency, set metrics_path to None\n",
    "collect_env._metrics_path = None\n",
    "collect_env._occupancy_normalization_constant = 125.0\n",
    "\n",
    "eval_env = load_environment(eval_scenario_config)\n",
    "# eval_env._label += \"_eval\"\n",
    "eval_env._metrics_path = metrics_path\n",
    "eval_env._occupancy_normalization_constant = 125.0\n",
    "\n",
    "initial_collect_env = load_environment(eval_scenario_config)\n",
    "\n",
    "initial_collect_env._metrics_path = metrics_path\n",
    "initial_collect_env._occupancy_normalization_constant = 125.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c55CehnYR8lY"
   },
   "source": [
    "In the section below, we'll define a function that accepts the envirnment and a policy, and runs a fixed number of episodes. The policy can be a rules-based policy or an RL-based policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "bitzHo5_UbXy"
   },
   "outputs": [],
   "source": [
    "# @title Define a method to execute the policy on the environment.\n",
    "\n",
    "\n",
    "def get_trajectory(time_step, current_action: policy_step.PolicyStep):\n",
    "  \"\"\"Get the trajectory for the current action and time step.\"\"\"\n",
    "  observation = time_step.observation\n",
    "  action = current_action.action\n",
    "  policy_info = ()\n",
    "  reward = time_step.reward\n",
    "  discount = time_step.discount\n",
    "\n",
    "  if time_step.is_first():\n",
    "    traj = trajectory.first(observation, action, policy_info, reward, discount)\n",
    "\n",
    "  elif time_step.is_last():\n",
    "    traj = trajectory.last(observation, action, policy_info, reward, discount)\n",
    "\n",
    "  else:\n",
    "    traj = trajectory.mid(observation, action, policy_info, reward, discount)\n",
    "  return traj\n",
    "\n",
    "\n",
    "def compute_avg_return(\n",
    "    environment,\n",
    "    policy,\n",
    "    num_episodes=1,\n",
    "    time_zone: str = \"US/Pacific\",\n",
    "    render_interval_steps: int = 24,\n",
    "    trajectory_observers=None,\n",
    "):\n",
    "  \"\"\"Computes the average return of the policy on the environment.\n",
    "\n",
    "  Args:\n",
    "    environment: environment.Environment\n",
    "    policy: policy.Policy\n",
    "    num_episodes: total number of eposides to run.\n",
    "    time_zone: time zone of the environment\n",
    "    render_interval_steps: Number of steps to take between rendering.\n",
    "    trajectory_observers: list of trajectory observers for use in rendering.\n",
    "  \"\"\"\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "\n",
    "    episode_return = 0.0\n",
    "    t0 = time.time()\n",
    "    epoch = t0\n",
    "\n",
    "    step_id = 0\n",
    "    execution_times = []\n",
    "\n",
    "    while not time_step.is_last():\n",
    "\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "\n",
    "      if trajectory_observers is not None:\n",
    "        traj = get_trajectory(time_step, action_step)\n",
    "        for observer in trajectory_observers:\n",
    "          observer(traj)\n",
    "\n",
    "      episode_return += time_step.reward\n",
    "      t1 = time.time()\n",
    "      dt = t1 - t0\n",
    "      episode_seconds = t1 - epoch\n",
    "      execution_times.append(dt)\n",
    "      sim_time = environment.current_simulation_timestamp.tz_convert(time_zone)\n",
    "\n",
    "      print(\n",
    "          \"Step %5d Sim Time: %s, Reward: %8.2f, Return: %8.2f, Mean Step Time:\"\n",
    "          \" %8.2f s, Episode Time: %8.2f s\"\n",
    "          % (\n",
    "              step_id,\n",
    "              sim_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "              time_step.reward,\n",
    "              episode_return,\n",
    "              np.mean(execution_times),\n",
    "              episode_seconds,\n",
    "          )\n",
    "      )\n",
    "\n",
    "      if (step_id > 0) and (step_id % render_interval_steps == 0):\n",
    "        if environment._metrics_path:\n",
    "          clear_output(wait=True)\n",
    "          reader = get_latest_episode_reader(environment._metrics_path)\n",
    "          plot_timeseries_charts(reader, time_zone)\n",
    "        render_env(environment)\n",
    "\n",
    "      t0 = t1\n",
    "      step_id += 1\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86IIF7FrfJ_2"
   },
   "source": [
    "# Rules-based Control (RBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "X9JR8qze6Yvb"
   },
   "outputs": [],
   "source": [
    "# @title Utils for RBC\n",
    "\n",
    "# We're concerned with controlling Heatpumps/ACs and Hot Water Systems (HWS).\n",
    "class DeviceType(enum.Enum):\n",
    "  AC = 0\n",
    "  HWS = 1\n",
    "\n",
    "\n",
    "SetpointName = str  # Identify the setpoint\n",
    "# Setpoint value.\n",
    "SetpointValue = Union[float, int, bool]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScheduleEvent:\n",
    "  start_time: pd.Timedelta\n",
    "  device: DeviceType\n",
    "  setpoint_name: SetpointName\n",
    "  setpoint_value: SetpointValue\n",
    "\n",
    "\n",
    "# A schedule is a list of times and setpoints for a device.\n",
    "Schedule = list[ScheduleEvent]\n",
    "ActionSequence = list[tuple[DeviceType, SetpointName]]\n",
    "\n",
    "\n",
    "def to_rad(sin_theta: float, cos_theta: float) -> float:\n",
    "  \"\"\"Converts a sin and cos theta to radians to extract the time.\"\"\"\n",
    "\n",
    "  if sin_theta >= 0 and cos_theta >= 0:\n",
    "    return np.arccos(cos_theta)\n",
    "  elif sin_theta >= 0 and cos_theta < 0:\n",
    "    return np.pi - np.arcsin(sin_theta)\n",
    "  elif sin_theta < 0 and cos_theta < 0:\n",
    "    return np.pi - np.arcsin(sin_theta)\n",
    "  else:\n",
    "    return 2 * np.pi - np.arccos(cos_theta)\n",
    "\n",
    "  return np.arccos(cos_theta) + rad_offset\n",
    "\n",
    "\n",
    "def to_dow(sin_theta: float, cos_theta: float) -> float:\n",
    "  \"\"\"Converts a sin and cos theta to days to extract day of week.\"\"\"\n",
    "  theta = to_rad(sin_theta, cos_theta)\n",
    "  return np.floor(7 * theta / 2 / np.pi)\n",
    "\n",
    "\n",
    "def to_hod(sin_theta: float, cos_theta: float) -> float:\n",
    "  \"\"\"Converts a sin and cos theta to hours to extract hour of day.\"\"\"\n",
    "  theta = to_rad(sin_theta, cos_theta)\n",
    "  return np.floor(24 * theta / 2 / np.pi)\n",
    "\n",
    "\n",
    "def find_schedule_action(\n",
    "    schedule: Schedule,\n",
    "    device: DeviceType,\n",
    "    setpoint_name: SetpointName,\n",
    "    timestamp: pd.Timedelta,\n",
    ") -> SetpointValue:\n",
    "  \"\"\"Finds the action for a schedule event for a time and schedule.\"\"\"\n",
    "\n",
    "  # Get all the schedule events for the device and the setpoint, and turn it\n",
    "  # into a series.\n",
    "  device_schedule_dict = {}\n",
    "  for schedule_event in schedule:\n",
    "    if (\n",
    "        schedule_event.device == device\n",
    "        and schedule_event.setpoint_name == setpoint_name\n",
    "    ):\n",
    "      device_schedule_dict[schedule_event.start_time] = (\n",
    "          schedule_event.setpoint_value\n",
    "      )\n",
    "  device_schedule = pd.Series(device_schedule_dict)\n",
    "\n",
    "  # Get the indexes of the schedule events that fall before the timestamp.\n",
    "\n",
    "  device_schedule_indexes = device_schedule.index[\n",
    "      device_schedule.index <= timestamp\n",
    "  ]\n",
    "\n",
    "  # If are no events preceedding the time, then choose the last\n",
    "  # (assuming it wraps around).\n",
    "  if device_schedule_indexes.empty:\n",
    "    return device_schedule.loc[device_schedule.index[-1]]\n",
    "  else:\n",
    "    return device_schedule.loc[device_schedule_indexes[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "QZON8t8w2KF5"
   },
   "outputs": [],
   "source": [
    "# @title Define a schedule policy\n",
    "\n",
    "class SchedulePolicy(tf_policy.TFPolicy):\n",
    "  \"\"\"TF Policy implementation of the Schedule policy.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      time_step_spec,\n",
    "      action_spec: types.NestedTensorSpec,\n",
    "      action_sequence: ActionSequence,\n",
    "      weekday_schedule_events: Schedule,\n",
    "      weekend_holiday_schedule_events: Schedule,\n",
    "      dow_sin_index: int,\n",
    "      dow_cos_index: int,\n",
    "      hod_sin_index: int,\n",
    "      hod_cos_index: int,\n",
    "      action_normalizers,\n",
    "      local_start_time: str = pd.Timestamp,\n",
    "      policy_state_spec: types.NestedTensorSpec = (),\n",
    "      info_spec: types.NestedTensorSpec = (),\n",
    "      training: bool = False,\n",
    "      name: Optional[str] = None,\n",
    "  ):\n",
    "    self.weekday_schedule_events = weekday_schedule_events\n",
    "    self.weekend_holiday_schedule_events = weekend_holiday_schedule_events\n",
    "    self.dow_sin_index = dow_sin_index\n",
    "    self.dow_cos_index = dow_cos_index\n",
    "    self.hod_sin_index = hod_sin_index\n",
    "    self.hod_cos_index = hod_cos_index\n",
    "    self.action_sequence = action_sequence\n",
    "    self.action_normalizers = action_normalizers\n",
    "    self.local_start_time = local_start_time\n",
    "    self.norm_mean = 0.0\n",
    "    self.norm_std = 1.0\n",
    "\n",
    "    policy_state_spec = ()\n",
    "\n",
    "    super().__init__(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        policy_state_spec=policy_state_spec,\n",
    "        info_spec=info_spec,\n",
    "        clip=False,\n",
    "        observation_and_action_constraint_splitter=None,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "  def _normalize_action_map(\n",
    "      self, action_map: dict[tuple[DeviceType, SetpointName], SetpointValue]\n",
    "  ) -> dict[tuple[DeviceType, SetpointName], SetpointValue]:\n",
    "\n",
    "    normalized_action_map = {}\n",
    "\n",
    "    for k, v in action_map.items():\n",
    "      for normalizer_k, normalizer in self.action_normalizers.items():\n",
    "        if normalizer_k.endswith(k[1]):\n",
    "\n",
    "          normed_v = normalizer.agent_value(v)\n",
    "          normalized_action_map[k] = normed_v\n",
    "\n",
    "    return normalized_action_map\n",
    "\n",
    "  def _get_action(\n",
    "      self, time_step\n",
    "  ) -> dict[tuple[DeviceType, SetpointName], SetpointValue]:\n",
    "\n",
    "    observation = time_step.observation\n",
    "    action_spec = cast(tensor_spec.BoundedTensorSpec, self.action_spec)\n",
    "    dow_sin = (observation[self.dow_sin_index] * self.norm_std) + self.norm_mean\n",
    "    dow_cos = (observation[self.dow_cos_index] * self.norm_std) + self.norm_mean\n",
    "    hod_sin = (observation[self.hod_sin_index] * self.norm_std) + self.norm_mean\n",
    "    hod_cos = (observation[self.hod_cos_index] * self.norm_std) + self.norm_mean\n",
    "\n",
    "    dow = to_dow(dow_sin, dow_cos)\n",
    "    hod = to_hod(hod_sin, hod_cos)\n",
    "\n",
    "    timestamp = (\n",
    "        pd.Timedelta(hod, unit='hour') + self.local_start_time.utcoffset()\n",
    "    )\n",
    "\n",
    "    if dow < 5:  # weekday\n",
    "\n",
    "      action_map = {\n",
    "          (tup[0], tup[1]): find_schedule_action(\n",
    "              self.weekday_schedule_events, tup[0], tup[1], timestamp\n",
    "          )\n",
    "          for tup in action_sequence\n",
    "      }\n",
    "\n",
    "      return action_map\n",
    "\n",
    "    else:  # Weekend\n",
    "\n",
    "      action_map = {\n",
    "          (tup[0], tup[1]): find_schedule_action(\n",
    "              self.weekend_holiday_schedule_events, tup[0], tup[1], timestamp\n",
    "          )\n",
    "          for tup in action_sequence\n",
    "      }\n",
    "\n",
    "      return action_map\n",
    "\n",
    "  def _action(self, time_step, policy_state, seed):\n",
    "    del seed\n",
    "    action_map = self._get_action(time_step)\n",
    "    normalized_action_map = self._normalize_action_map(action_map)\n",
    "\n",
    "    action = np.array(\n",
    "        [\n",
    "            normalized_action_map[device_setpoint]\n",
    "            for device_setpoint in action_sequence\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    t_action = tf.convert_to_tensor(action)\n",
    "    return policy_step.PolicyStep(t_action, (), ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkQs64KT6qs-"
   },
   "source": [
    "Next, we parameterize the setpoint schedule.\n",
    "\n",
    "We distinguish between weekend and holiday schedules:\n",
    "\n",
    "* For **weekdays, between 6:00 am and 7:00 pm local time** we maintain occupancy conditions:\n",
    "  * AC/Heatpump supply air heating setpoint is 12 C\n",
    "  * Supply water temperarure is 77 C\n",
    "* For **weekday, before 6:00 am and after 7:00 pm locl time** we maintain efficiency conditions (setback):\n",
    "  * AC/Heatpump supply air heating setpoint is 0 C\n",
    "  * Supply water temperarure is 42 C\n",
    "\n",
    "* For **weekends and holdidays**, all day, we maintain efficiency conditions (setback):\n",
    "  * AC/Heatpump supply air heating setpoint is 0 C\n",
    "  * Supply water temperarure is 42 C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "id": "SpveeGWUf5AK"
   },
   "outputs": [],
   "source": [
    "# @title Configure the schedule parameters\n",
    "\n",
    "hod_cos_index = collect_env._field_names.index('hod_cos_000')\n",
    "hod_sin_index = collect_env._field_names.index('hod_sin_000')\n",
    "dow_cos_index = collect_env._field_names.index('dow_cos_000')\n",
    "dow_sin_index = collect_env._field_names.index('dow_sin_000')\n",
    "\n",
    "\n",
    "# Note that temperatures are specified in Kelvin:\n",
    "weekday_schedule_events = [\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(6, unit='hour'),\n",
    "        DeviceType.AC,\n",
    "        'supply_air_heating_temperature_setpoint',\n",
    "        292.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(19, unit='hour'),\n",
    "        DeviceType.AC,\n",
    "        'supply_air_heating_temperature_setpoint',\n",
    "        285.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(6, unit='hour'),\n",
    "        DeviceType.HWS,\n",
    "        'supply_water_setpoint',\n",
    "        350.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(19, unit='hour'),\n",
    "        DeviceType.HWS,\n",
    "        'supply_water_setpoint',\n",
    "        315.0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "weekend_holiday_schedule_events = [\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(6, unit='hour'),\n",
    "        DeviceType.AC,\n",
    "        'supply_air_heating_temperature_setpoint',\n",
    "        285.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(19, unit='hour'),\n",
    "        DeviceType.AC,\n",
    "        'supply_air_heating_temperature_setpoint',\n",
    "        285.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(6, unit='hour'),\n",
    "        DeviceType.HWS,\n",
    "        'supply_water_setpoint',\n",
    "        315.0,\n",
    "    ),\n",
    "    ScheduleEvent(\n",
    "        pd.Timedelta(19, unit='hour'),\n",
    "        DeviceType.HWS,\n",
    "        'supply_water_setpoint',\n",
    "        315.0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "action_sequence = [\n",
    "    (DeviceType.HWS, 'supply_water_setpoint'),\n",
    "    (DeviceType.AC, 'supply_air_heating_temperature_setpoint'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOTP9p8-0N0H"
   },
   "source": [
    "We instantiate the schedule policy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "jv-1GBFTieNc"
   },
   "outputs": [],
   "source": [
    "# @title Instantiate the Schedule RBC policy\n",
    "ts = collect_env.reset()\n",
    "local_start_time = collect_env.current_simulation_timestamp.tz_convert(tz = 'US/Pacific')\n",
    "\n",
    "action_normalizers = collect_env._action_normalizers\n",
    "\n",
    "observation_spec, action_spec, time_step_spec = spec_utils.get_tensor_specs(collect_env)\n",
    "schedule_policy = SchedulePolicy(\n",
    "    time_step_spec= time_step_spec,\n",
    "    action_spec= action_spec,\n",
    "    action_sequence = action_sequence,\n",
    "    weekday_schedule_events = weekday_schedule_events,\n",
    "    weekend_holiday_schedule_events = weekend_holiday_schedule_events,\n",
    "    dow_sin_index=dow_sin_index,\n",
    "    dow_cos_index=dow_cos_index,\n",
    "    hod_sin_index=hod_sin_index,\n",
    "    hod_cos_index=hod_cos_index,\n",
    "    local_start_time=local_start_time,\n",
    "    action_normalizers=action_normalizers,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAYOf5Xtzi2u"
   },
   "source": [
    "Next, we will run the static control setpoints on the environment to establish baseline performance.\n",
    "\n",
    "**Note:** This will take some time to execute. Feel free to skip this step if you want to jump directly to the RL section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "3Zv-lSiWDp50"
   },
   "outputs": [],
   "source": [
    "# @title Optionally, execute the schedule policy on the environment\n",
    "# Optional\n",
    "# compute_avg_return(eval_env, schedule_policy, 1, time_zone=\"US/Pacific\", render_interval_steps=144, trajectory_observers=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDgizVLzRti1"
   },
   "source": [
    "# Reinforcement Learning Control\n",
    "In the previous section we used a simple schedule to control the HVAC setpoints, however in this section, we configure and train a Reinforcement Learning (RL) agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "zBjFBpkabFHR"
   },
   "outputs": [],
   "source": [
    "# @title Utilities to configure networks for the RL Agent.\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer='glorot_uniform',\n",
    ")\n",
    "\n",
    "\n",
    "def logging_info(*args):\n",
    "  logging.info(*args)\n",
    "  print(*args)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "  return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "  return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_sequential_critic_network(\n",
    "    obs_fc_layer_units, action_fc_layer_units, joint_fc_layer_units\n",
    "):\n",
    "  \"\"\"Create a sequential critic network.\"\"\"\n",
    "\n",
    "  # Split the inputs into observations and actions.\n",
    "  def split_inputs(inputs):\n",
    "    return {'observation': inputs[0], 'action': inputs[1]}\n",
    "\n",
    "  # Create an observation network.\n",
    "  obs_network = (\n",
    "      create_fc_network(obs_fc_layer_units)\n",
    "      if obs_fc_layer_units\n",
    "      else create_identity_layer()\n",
    "  )\n",
    "\n",
    "  # Create an action network.\n",
    "  action_network = (\n",
    "      create_fc_network(action_fc_layer_units)\n",
    "      if action_fc_layer_units\n",
    "      else create_identity_layer()\n",
    "  )\n",
    "\n",
    "  # Create a joint network.\n",
    "  joint_network = (\n",
    "      create_fc_network(joint_fc_layer_units)\n",
    "      if joint_fc_layer_units\n",
    "      else create_identity_layer()\n",
    "  )\n",
    "\n",
    "  # Final layer.\n",
    "  value_layer = tf.keras.layers.Dense(1, kernel_initializer='glorot_uniform')\n",
    "\n",
    "  return sequential.Sequential(\n",
    "      [\n",
    "          tf.keras.layers.Lambda(split_inputs),\n",
    "          nest_map.NestMap(\n",
    "              {'observation': obs_network, 'action': action_network}\n",
    "          ),\n",
    "          nest_map.NestFlatten(),\n",
    "          tf.keras.layers.Concatenate(),\n",
    "          joint_network,\n",
    "          value_layer,\n",
    "          inner_reshape.InnerReshape(current_shape=[1], new_shape=[]),\n",
    "      ],\n",
    "      name='sequential_critic',\n",
    "  )\n",
    "\n",
    "\n",
    "class _TanhNormalProjectionNetworkWrapper(\n",
    "    tanh_normal_projection_network.TanhNormalProjectionNetwork\n",
    "):\n",
    "  \"\"\"Wrapper to pass predefined `outer_rank` to underlying projection net.\"\"\"\n",
    "\n",
    "  def __init__(self, sample_spec, predefined_outer_rank=1):\n",
    "    super(_TanhNormalProjectionNetworkWrapper, self).__init__(sample_spec)\n",
    "    self.predefined_outer_rank = predefined_outer_rank\n",
    "\n",
    "  def call(self, inputs, network_state=(), **kwargs):\n",
    "    kwargs['outer_rank'] = self.predefined_outer_rank\n",
    "    if 'step_type' in kwargs:\n",
    "      del kwargs['step_type']\n",
    "    return super(_TanhNormalProjectionNetworkWrapper, self).call(\n",
    "        inputs, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def create_sequential_actor_network(actor_fc_layers, action_tensor_spec):\n",
    "  \"\"\"Create a sequential actor network.\"\"\"\n",
    "\n",
    "  def tile_as_nest(non_nested_output):\n",
    "    return tf.nest.map_structure(\n",
    "        lambda _: non_nested_output, action_tensor_spec\n",
    "    )\n",
    "\n",
    "  return sequential.Sequential(\n",
    "      [dense(num_units) for num_units in actor_fc_layers]\n",
    "      + [tf.keras.layers.Lambda(tile_as_nest)]\n",
    "      + [\n",
    "          nest_map.NestMap(\n",
    "              tf.nest.map_structure(\n",
    "                  _TanhNormalProjectionNetworkWrapper, action_tensor_spec\n",
    "              )\n",
    "          )\n",
    "      ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g6pE6v2bb8O"
   },
   "source": [
    "Set the configuration parameters for the SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "CeVkerwYcng2"
   },
   "outputs": [],
   "source": [
    "# @title Set the RL Agent's parameters\n",
    "\n",
    "# Actor network fully connected layers.\n",
    "actor_fc_layers = (128, 128)\n",
    "# Critic network observation fully connected layers.\n",
    "critic_obs_fc_layers = (128, 64)\n",
    "# Critic network action fully connected layers.\n",
    "critic_action_fc_layers = (128, 64)\n",
    "# Critic network joint fully connected layers.\n",
    "critic_joint_fc_layers = (128, 64)\n",
    "\n",
    "batch_size = 256\n",
    "actor_learning_rate = 3e-4\n",
    "critic_learning_rate = 3e-4\n",
    "alpha_learning_rate = 3e-4\n",
    "gamma = 0.99\n",
    "target_update_tau= 0.005\n",
    "target_update_period= 1\n",
    "reward_scale_factor = 1.0\n",
    "\n",
    "# Replay params\n",
    "replay_capacity = 1000000\n",
    "debug_summaries = True\n",
    "summarize_grads_and_vars = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhTPXjtebMZD"
   },
   "source": [
    "## Initialize the SAC agent\n",
    "\n",
    "Of all the Reinforcement learning algorithms, we have chosen [Soft Actor Cirtic (SAC)](https://arxiv.org/abs/1801.01290) because its proven performance on evironments with  high-dimensional states and real-valued actions.\n",
    "\n",
    "In this notebook we illustrate the use of the buidling control environment using the SAC implementation in [TF-Agents](https://www.tensorflow.org/agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "NW0pzLvjbSnP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:25:10.984536: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# @title Construct the SAC agent\n",
    "\n",
    "_, action_tensor_spec, time_step_tensor_spec = spec_utils.get_tensor_specs(\n",
    "    collect_env\n",
    ")\n",
    "\n",
    "actor_net = create_sequential_actor_network(\n",
    "    actor_fc_layers=actor_fc_layers, action_tensor_spec=action_tensor_spec\n",
    ")\n",
    "\n",
    "critic_net = create_sequential_critic_network(\n",
    "    obs_fc_layer_units=critic_obs_fc_layers,\n",
    "    action_fc_layer_units=critic_action_fc_layers,\n",
    "    joint_fc_layer_units=critic_joint_fc_layers,\n",
    ")\n",
    "\n",
    "\n",
    "train_step = train_utils.create_train_step()\n",
    "agent = sac_agent.SacAgent(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.keras.optimizers.Adam(learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=critic_learning_rate\n",
    "    ),\n",
    "    alpha_optimizer=tf.keras.optimizers.Adam(learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.math.squared_difference,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=None,\n",
    "    debug_summaries=debug_summaries,\n",
    "    summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "    train_step_counter=train_step,\n",
    ")\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5hNdgZBG5BZ"
   },
   "source": [
    "Below we construct a replay buffer using reverb. The replay buffer is popualted with state-action-reward-state tuples during collect. Thie allows the agent to relive past experiences, and prevents the model from overfitting in the local neighborhood.\n",
    "\n",
    "During traning, the agent samples from the replay buffer. This helps decorrelate the traiing data in a way that randomization of a training set would in supervised learning. Otherwise, in most environments the experience in a window of time is highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "vX2zGUWJGWAl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverb_checkpoint_dir=/home/vaibhav/sbsim_model_based/output/reverb_checkpoint\n",
      "reverb_server_port=38017\n",
      "num_frames in replay buffer=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /home/vaibhav/sbsim_model_based/output/reverb_checkpoint.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /home/vaibhav/sbsim_model_based/output/reverb_checkpoint\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 38017\n"
     ]
    }
   ],
   "source": [
    "# @title Set up the replay buffer\n",
    "replay_capacity = 50000\n",
    "table_name = 'uniform_table'\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    ")\n",
    "\n",
    "reverb_checkpoint_dir = output_data_path + \"/reverb_checkpoint\"\n",
    "reverb_port = None\n",
    "print('reverb_checkpoint_dir=%s' %reverb_checkpoint_dir)\n",
    "reverb_checkpointer = reverb.platform.checkpointers_lib.DefaultCheckpointer(\n",
    "    path=reverb_checkpoint_dir\n",
    ")\n",
    "reverb_server = reverb.Server(\n",
    "    [table], port=reverb_port, checkpointer=reverb_checkpointer\n",
    ")\n",
    "logging_info('reverb_server_port=%d' %reverb_server.port)\n",
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    sequence_length=2,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server,\n",
    ")\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "    reverb_replay.py_client, table_name, sequence_length=2, stride_length=1\n",
    ")\n",
    "print('num_frames in replay buffer=%d' %reverb_replay.num_frames())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH7LQZ_Pd0vY"
   },
   "source": [
    "For simplicity, we'll grab eval and collact policies and give them short variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "BwY7StuMkuV4"
   },
   "outputs": [],
   "source": [
    "# @title Access the eval and collect policies\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6klSPQeGsPLz"
   },
   "source": [
    "In the next section we define observer classes that enable printing model and environment output as the scenario evolves to who you the percentage of the episode, the timestamp in the scenario, cumulative reward, and the execution time.\n",
    "\n",
    "We also provide a plot observer that periodically outputs the performance charts and the temperature gradient across both floors of the buidling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "dJ_EMQkZdw8q"
   },
   "outputs": [],
   "source": [
    "# @title Define Observers\n",
    "class RenderAndPlotObserver:\n",
    "  \"\"\"Renders and plots the environment.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      render_interval_steps: int = 10,\n",
    "      environment=None,\n",
    "  ):\n",
    "    self._counter = 0\n",
    "    self._render_interval_steps = render_interval_steps\n",
    "    self._environment = environment\n",
    "    self._cumulative_reward = 0.0\n",
    "\n",
    "    self._start_time = None\n",
    "    if self._environment is not None:\n",
    "      self._num_timesteps_in_episode = (\n",
    "          self._environment._num_timesteps_in_episode\n",
    "      )\n",
    "      self._environment._end_timestamp\n",
    "\n",
    "  def __call__(self, trajectory: trajectory_lib.Trajectory) -> None:\n",
    "\n",
    "    reward = trajectory.reward\n",
    "    self._cumulative_reward += reward\n",
    "    self._counter += 1\n",
    "    if self._start_time is None:\n",
    "      self._start_time = pd.Timestamp.now()\n",
    "\n",
    "    if self._counter % self._render_interval_steps == 0 and self._environment:\n",
    "\n",
    "      execution_time = pd.Timestamp.now() - self._start_time\n",
    "      mean_execution_time = execution_time.total_seconds() / self._counter\n",
    "\n",
    "      clear_output(wait=True)\n",
    "      if self._environment._metrics_path is not None:\n",
    "        reader = get_latest_episode_reader(self._environment._metrics_path)\n",
    "        plot_timeseries_charts(reader, time_zone)\n",
    "\n",
    "      render_env(self._environment)\n",
    "\n",
    "\n",
    "class PrintStatusObserver:\n",
    "  \"\"\"Prints status information.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, status_interval_steps: int = 1, environment=None, replay_buffer=None\n",
    "  ):\n",
    "    self._counter = 0\n",
    "    self._status_interval_steps = status_interval_steps\n",
    "    self._environment = environment\n",
    "    self._cumulative_reward = 0.0\n",
    "    self._replay_buffer = replay_buffer\n",
    "\n",
    "    self._start_time = None\n",
    "    if self._environment is not None:\n",
    "      self._num_timesteps_in_episode = (\n",
    "          self._environment._num_timesteps_in_episode\n",
    "      )\n",
    "      self._environment._end_timestamp\n",
    "\n",
    "  def __call__(self, trajectory: trajectory_lib.Trajectory) -> None:\n",
    "\n",
    "    reward = trajectory.reward\n",
    "    self._cumulative_reward += reward\n",
    "    self._counter += 1\n",
    "    if self._start_time is None:\n",
    "      self._start_time = pd.Timestamp.now()\n",
    "\n",
    "    if self._counter % self._status_interval_steps == 0 and self._environment:\n",
    "\n",
    "      execution_time = pd.Timestamp.now() - self._start_time\n",
    "      mean_execution_time = execution_time.total_seconds() / self._counter\n",
    "\n",
    "      sim_time = self._environment.current_simulation_timestamp.tz_convert(\n",
    "          time_zone\n",
    "      )\n",
    "      percent_complete = int(\n",
    "          100.0 * (self._counter / self._num_timesteps_in_episode)\n",
    "      )\n",
    "\n",
    "      if self._replay_buffer is not None:\n",
    "        rb_size = self._replay_buffer.num_frames()\n",
    "        rb_string = \" Replay Buffer Size: %d\" % rb_size\n",
    "      else:\n",
    "        rb_string = \"\"\n",
    "\n",
    "      print(\n",
    "          \"Step %5d of %5d (%3d%%) Sim Time: %s Reward: %2.2f Cumulative\"\n",
    "          \" Reward: %8.2f Execution Time: %s Mean Execution Time: %3.2fs %s\"\n",
    "          % (\n",
    "              self._environment._step_count,\n",
    "              self._num_timesteps_in_episode,\n",
    "              percent_complete,\n",
    "              sim_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "              reward,\n",
    "              self._cumulative_reward,\n",
    "              execution_time,\n",
    "              mean_execution_time,\n",
    "              rb_string,\n",
    "          )\n",
    "      )\n",
    "\n",
    "\n",
    "initial_collect_render_plot_observer = RenderAndPlotObserver(\n",
    "    render_interval_steps=144, environment=initial_collect_env\n",
    ")\n",
    "initial_collect_print_status_observer = PrintStatusObserver(\n",
    "    status_interval_steps=1,\n",
    "    environment=initial_collect_env,\n",
    "    replay_buffer=reverb_replay,\n",
    ")\n",
    "collect_render_plot_observer = RenderAndPlotObserver(\n",
    "    render_interval_steps=144, environment=collect_env\n",
    ")\n",
    "collect_print_status_observer = PrintStatusObserver(\n",
    "    status_interval_steps=1,\n",
    "    environment=collect_env,\n",
    "    replay_buffer=reverb_replay,\n",
    ")\n",
    "eval_render_plot_observer = RenderAndPlotObserver(\n",
    "    render_interval_steps=144, environment=eval_env\n",
    ")\n",
    "eval_print_status_observer = PrintStatusObserver(\n",
    "    status_interval_steps=1, environment=eval_env, replay_buffer=reverb_replay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el732oZItQjO"
   },
   "source": [
    "In the following cell, we shall run the baseline control on the scenario to populate the replay buffer. We will use the schedule policy we build above to simulate training off-policy from recorded telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "ZGq3SY0kKwsa"
   },
   "outputs": [],
   "source": [
    "# @title Populate the replay buffer with data from baseline control\n",
    "# initial_collect_actor = actor.Actor(\n",
    "#   initial_collect_env,\n",
    "#   schedule_policy,\n",
    "#   train_step,\n",
    "#   steps_per_run=initial_collect_env._num_timesteps_in_episode,\n",
    "#   observers=[rb_observer, initial_collect_print_status_observer, initial_collect_render_plot_observer])\n",
    "# initial_collect_actor.run()\n",
    "# reverb_replay.py_client.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3ZzWxqIunCz"
   },
   "source": [
    "Next wrap the replay buffer into a TF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "id": "ba7bilizt_qW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (108397) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (108397) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (108397) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (108397) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (108397) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (108397) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    }
   ],
   "source": [
    "# @title Make a TF Dataset\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = reverb_replay.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(50)\n",
    "\n",
    "for experience_batch, _ in dataset.take(1):  # Take one batch from the dataset\n",
    "    next_step_types = experience_batch.next_step_type  # Access the next_step_type\n",
    "    print(next_step_types.numpy()) \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YqfMl5FuQpf"
   },
   "source": [
    "Here, we extract the collect and evaluation policies for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "id": "TzwSaxYkeTh5"
   },
   "outputs": [],
   "source": [
    "# @title Convert the policies into TF Eager Policies\n",
    "\n",
    "tf_collect_policy = agent.collect_policy\n",
    "\n",
    "\n",
    "agent_collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)\n",
    "\n",
    "tf_policy = agent.policy\n",
    "agent_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtoqyo8Ypn0Q"
   },
   "source": [
    "We will set the interval of saving the policies and writing critic, actor, and alphs losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xums9Kxkxylw"
   },
   "outputs": [],
   "source": [
    "policy_save_interval = 1 # Save the policy after every learning step.\n",
    "learner_summary_interval = 1 # Produce a summary of the critic, actor, and alpha losses after every gradient update step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al5HNoiwvYO-"
   },
   "source": [
    "In the following cell we will define the agent learner, a TF-Agents wrapper around the process that performs gradiant-based updates to the actor and critic networks in the agent.\n",
    "\n",
    "You should see a statememt that shows you where the policies will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "id": "Ah4oS9HLwOid"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies will be saved to saved_model_dir: /home/vaibhav/sbsim_model_based/root/policies\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "this __dict__ descriptor does not support '_DictWrapper' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolicies will be saved to saved_model_dir: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39msaved_model_dir)\n\u001b[1;32m      7\u001b[0m env_step_metric \u001b[38;5;241m=\u001b[39m py_metrics\u001b[38;5;241m.\u001b[39mEnvironmentSteps()\n\u001b[1;32m      8\u001b[0m learning_triggers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 9\u001b[0m       \u001b[43mtriggers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPolicySavedModelTrigger\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43msaved_model_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_save_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmetadata_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mtriggers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mENV_STEP_METADATA_KEY\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_step_metric\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     16\u001b[0m       triggers\u001b[38;5;241m.\u001b[39mStepPerSecondLogTrigger(train_step, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m agent_learner \u001b[38;5;241m=\u001b[39m learner\u001b[38;5;241m.\u001b[39mLearner(\n\u001b[1;32m     20\u001b[0m       root_dir,\n\u001b[1;32m     21\u001b[0m       train_step,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m       summary_interval\u001b[38;5;241m=\u001b[39mlearner_summary_interval,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tf_agents/train/triggers.py:129\u001b[0m, in \u001b[0;36mPolicySavedModelTrigger.__init__\u001b[0;34m(self, saved_model_dir, agent, train_step, interval, async_saving, metadata_metrics, start, extra_concrete_functions, batch_size, use_nest_path_signatures, save_greedy_policy, save_collect_policy, input_fn_and_spec)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m save_greedy_policy:\n\u001b[1;32m    127\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m greedy_policy\u001b[38;5;241m.\u001b[39mGreedyPolicy(agent\u001b[38;5;241m.\u001b[39mpolicy)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_policy_saver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_saver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_nest_path_signatures\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m savers \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_policy_saver, learner\u001b[38;5;241m.\u001b[39mRAW_POLICY_SAVED_MODEL_DIR)]\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_collect_policy:\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tf_agents/train/triggers.py:177\u001b[0m, in \u001b[0;36mPolicySavedModelTrigger._build_saver\u001b[0;34m(self, policy, batch_size, use_nest_path_signatures)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_saver\u001b[39m(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    173\u001b[0m     policy: tf_policy\u001b[38;5;241m.\u001b[39mTFPolicy,\n\u001b[1;32m    174\u001b[0m     batch_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    175\u001b[0m     use_nest_path_signatures: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[policy_saver\u001b[38;5;241m.\u001b[39mPolicySaver, async_policy_saver\u001b[38;5;241m.\u001b[39mAsyncPolicySaver]:\n\u001b[0;32m--> 177\u001b[0m   saver \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPolicySaver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_nest_path_signatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nest_path_signatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_fn_and_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_fn_and_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_saving:\n\u001b[1;32m    186\u001b[0m     saver \u001b[38;5;241m=\u001b[39m async_policy_saver\u001b[38;5;241m.\u001b[39mAsyncPolicySaver(saver)\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tf_agents/policies/policy_saver.py:333\u001b[0m, in \u001b[0;36mPolicySaver.__init__\u001b[0;34m(self, policy, batch_size, use_nest_path_signatures, seed, train_step, input_fn_and_spec, metadata)\u001b[0m\n\u001b[1;32m    326\u001b[0m get_initial_state_fn\u001b[38;5;241m.\u001b[39mget_concrete_function(\u001b[38;5;241m*\u001b[39mget_initial_state_input_specs)\n\u001b[1;32m    328\u001b[0m train_step_fn \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: saved_policy\u001b[38;5;241m.\u001b[39mtrain_step\n\u001b[1;32m    330\u001b[0m )\u001b[38;5;241m.\u001b[39mget_concrete_function()\n\u001b[1;32m    331\u001b[0m get_metadata_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcommon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\n\u001b[0;32m--> 333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m batched_time_step_spec \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m spec: add_batch_dim(spec, [batch_size]), policy\u001b[38;5;241m.\u001b[39mtime_step_spec\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    338\u001b[0m batched_time_step_spec \u001b[38;5;241m=\u001b[39m cast(ts\u001b[38;5;241m.\u001b[39mTimeStep, batched_time_step_spec)\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1227\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1226\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1197\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1201\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:695\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    691\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    692\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    693\u001b[0m )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    700\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:331\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    328\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_captures\u001b[38;5;241m.\u001b[39mmerge_by_ref_with(graph_capture_container)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Create a new FunctionType including captures and outputs.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m output_type \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraced_func_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_context\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m traced_func_type \u001b[38;5;241m=\u001b[39m function_type_lib\u001b[38;5;241m.\u001b[39mFunctionType(\n\u001b[1;32m    335\u001b[0m     function_type\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    336\u001b[0m     traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\u001b[38;5;241m.\u001b[39mcapture_types,\n\u001b[1;32m    337\u001b[0m     return_annotation\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    340\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m concrete_function_lib\u001b[38;5;241m.\u001b[39mConcreteFunction\u001b[38;5;241m.\u001b[39mfrom_func_graph(\n\u001b[1;32m    341\u001b[0m     traced_func_graph,\n\u001b[1;32m    342\u001b[0m     traced_func_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m )\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:144\u001b[0m, in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mis_legacy_signature \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, trace\u001b[38;5;241m.\u001b[39mTraceType):\n\u001b[1;32m    143\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSupportsTracingProtocol\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m   generated_type \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39m__tf_tracing_type__(context)\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generated_type, trace\u001b[38;5;241m.\u001b[39mTraceType):\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/typing_extensions.py:647\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__protocol_attrs__:\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 647\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetattr_static\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffaug/lib/python3.10/inspect.py:1743\u001b[0m, in \u001b[0;36mgetattr_static\u001b[0;34m(obj, attr, default)\u001b[0m\n\u001b[1;32m   1740\u001b[0m     dict_attr \u001b[38;5;241m=\u001b[39m _shadowed_dict(klass)\n\u001b[1;32m   1741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (dict_attr \u001b[38;5;129;01mis\u001b[39;00m _sentinel \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m         \u001b[38;5;28mtype\u001b[39m(dict_attr) \u001b[38;5;129;01mis\u001b[39;00m types\u001b[38;5;241m.\u001b[39mMemberDescriptorType):\n\u001b[0;32m-> 1743\u001b[0m         instance_result \u001b[38;5;241m=\u001b[39m \u001b[43m_check_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     klass \u001b[38;5;241m=\u001b[39m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/diffaug/lib/python3.10/inspect.py:1690\u001b[0m, in \u001b[0;36m_check_instance\u001b[0;34m(obj, attr)\u001b[0m\n\u001b[1;32m   1688\u001b[0m instance_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1690\u001b[0m     instance_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__dict__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: this __dict__ descriptor does not support '_DictWrapper' objects"
     ]
    }
   ],
   "source": [
    "# @title Define an Agent Learner\n",
    "\n",
    "experience_dataset_fn = lambda: dataset\n",
    "\n",
    "saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "print('Policies will be saved to saved_model_dir: %s' %saved_model_dir)\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "learning_triggers = [\n",
    "      triggers.PolicySavedModelTrigger(\n",
    "          saved_model_dir,\n",
    "          agent,\n",
    "          train_step,\n",
    "          interval=policy_save_interval,\n",
    "          metadata_metrics={triggers.ENV_STEP_METADATA_KEY: env_step_metric},\n",
    "      ),\n",
    "      triggers.StepPerSecondLogTrigger(train_step, interval=10),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "      root_dir,\n",
    "      train_step,\n",
    "      agent,\n",
    "      experience_dataset_fn,\n",
    "      triggers=learning_triggers,\n",
    "      strategy=None,\n",
    "      summary_interval=learner_summary_interval,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAdbomqlyqpz"
   },
   "source": [
    "Set the number of training steps in a training iteration. This is the number of collect steps between gradient updates.\n",
    "\n",
    "Here we set the number of training steps to the length of a full episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6iWtSC-FKHMW"
   },
   "outputs": [],
   "source": [
    "collect_steps_per_treining_iteration = collect_env._num_timesteps_in_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdKA4Jy4YfJM"
   },
   "source": [
    "Next, we will define a *collect actor* and an *eval actor* that wrap the policy and the environment, and can execute and collect metrics.\n",
    "\n",
    "The principal difference between the collect actor and the eval actor, is that the collect actor will choose actions by drawing off the actor network distribution, choosing actions that have a high probability over actions with lower probability. This stochastic property enables the agent explore bettwer actions and improve the policy.\n",
    "\n",
    "However, the eval actor always chooses the action associated with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "id": "LWsI9znlqLvh"
   },
   "outputs": [],
   "source": [
    "# @title Define a TF-Agents Actor for collect and eval\n",
    "tf_collect_policy = agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "    tf_collect_policy, use_tf_function=True\n",
    ")\n",
    "collect_actor = actor.Actor(\n",
    "    collect_env,\n",
    "    collect_policy,\n",
    "    train_step,\n",
    "    steps_per_run=collect_steps_per_treining_iteration,\n",
    "    metrics=actor.collect_metrics(1),\n",
    "    summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
    "    summary_interval=1,\n",
    "    observers=[\n",
    "        rb_observer,\n",
    "        env_step_metric,\n",
    "        collect_print_status_observer,\n",
    "        collect_render_plot_observer,\n",
    "    ],\n",
    ")\n",
    "\n",
    "tf_greedy_policy = greedy_policy.GreedyPolicy(agent.policy)\n",
    "eval_greedy_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "    tf_greedy_policy, use_tf_function=True\n",
    ")\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "    eval_env,\n",
    "    eval_greedy_policy,\n",
    "    train_step,\n",
    "    episodes_per_run=1,\n",
    "    metrics=actor.eval_metrics(1),\n",
    "    summary_dir=os.path.join(root_dir, 'eval'),\n",
    "    summary_interval=1,\n",
    "    observers=[rb_observer, eval_print_status_observer, eval_render_plot_observer],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the World Model (Transformer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TransformerWorldModel(tf.keras.Model):\n",
    "    \"\"\"Transformer-based world model for predicting next states and rewards.\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim, \n",
    "        action_dim, \n",
    "        hidden_dim=256, \n",
    "        num_heads=8,\n",
    "        num_transformer_blocks=3,\n",
    "        ff_dim=512,\n",
    "        dropout_rate=0.1,\n",
    "        name=\"transformer_world_model\"\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.state_embedding = tf.keras.layers.Dense(\n",
    "            hidden_dim, \n",
    "            name=\"state_embedding\"\n",
    "        )\n",
    "        self.action_embedding = tf.keras.layers.Dense(\n",
    "            hidden_dim, \n",
    "            name=\"action_embedding\"\n",
    "        )\n",
    "        \n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=50,\n",
    "            output_dim=hidden_dim,\n",
    "            name=\"position_embedding\"\n",
    "        )\n",
    "        \n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(hidden_dim, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ]\n",
    "        \n",
    "        self.next_state_head = tf.keras.layers.Dense(\n",
    "            state_dim, \n",
    "            name=\"next_state_prediction\"\n",
    "        )\n",
    "        self.reward_head = tf.keras.layers.Dense(\n",
    "            1, \n",
    "            name=\"reward_prediction\"\n",
    "        )\n",
    "        \n",
    "        self.uncertainty_head = tf.keras.layers.Dense(\n",
    "            state_dim, \n",
    "            name=\"uncertainty_estimation\"\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        states, actions = inputs\n",
    "        \n",
    "        positions = tf.range(start=0, limit=tf.shape(states)[1], delta=1)\n",
    "        positions = tf.expand_dims(positions, axis=0)\n",
    "        \n",
    "        state_emb = self.state_embedding(states)  # [batch_size, seq_len, hidden_dim]\n",
    "        action_emb = self.action_embedding(actions)  # [batch_size, seq_len, hidden_dim]\n",
    "        pos_emb = self.position_embedding(positions)  # [1, seq_len, hidden_dim]\n",
    "        \n",
    "        x = state_emb + action_emb + pos_emb\n",
    "        \n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        \n",
    "        next_states = self.next_state_head(x)\n",
    "        rewards = self.reward_head(x)\n",
    "        uncertainties = self.uncertainty_head(x)\n",
    "        \n",
    "        \n",
    "        return next_states, rewards, uncertainties\n",
    "    \n",
    "    #make a very simple model instead of transformer\n",
    "\n",
    "class SimpleWorldModel(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, name=\"simple_world_model\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        states, actions = inputs\n",
    "        next_states = states + actions\n",
    "        rewards = tf.reduce_sum(states, axis=1)\n",
    "        uncertainties = tf.zeros_like(rewards)\n",
    "        return next_states, rewards, uncertainties\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the replay buffer to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAssistedReplayBuffer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        real_buffer: reverb.Client,\n",
    "        world_model: TransformerWorldModel,\n",
    "        rollout_length: int = 5,\n",
    "        rollout_ratio: float = 0.5,\n",
    "        uncertainty_threshold: float = 0.5\n",
    "    ):\n",
    "        self.real_buffer = real_buffer\n",
    "        self.world_model = world_model\n",
    "        self.rollout_length = rollout_length\n",
    "        self.rollout_ratio = rollout_ratio\n",
    "        self.uncertainty_threshold = uncertainty_threshold\n",
    "        \n",
    "    def generate_rollouts(self, initial_state, initial_reward, initial_discount, policy):\n",
    "        \"\"\"Generate model-based rollouts with uncertainty estimation.\"\"\"\n",
    "        # Convert initial state to tensor and add batch dimension\n",
    "        current_state = tf.expand_dims(tf.convert_to_tensor(initial_state, dtype=tf.float32), 0)\n",
    "        current_reward = initial_reward\n",
    "        current_discount = initial_discount\n",
    "        generated_experience = []\n",
    "\n",
    "        time_step = transition(np.array([current_state[0]], dtype=np.float32), reward=current_reward, discount=current_discount)\n",
    "        action_step = policy.action(time_step)\n",
    "        current_action = action_step.action\n",
    "\n",
    "        next_state, next_reward, uncertainty = self.world_model(\n",
    "                (current_state, current_action)\n",
    "            )\n",
    "        \n",
    "        for _ in range(self.rollout_length):\n",
    "            # Get action from policy\n",
    "            # print(\"Current state shape: \", current_state.shape)\n",
    "            # print(\"Current state[0] shape: \", current_state[0].shape)\n",
    "            # print(\"np array shape: \", np.array([current_state[0]], dtype=np.float32).shape)\n",
    "            time_step = transition(np.array([next_state[0]], dtype=np.float32), reward=next_reward, discount=current_discount)\n",
    "            # print(\"Time step observation shape: \", time_step.observation.shape)\n",
    "            # print(\"Time step reward: \", time_step.reward)\n",
    "            # print(\"Time step discount: \", time_step.discount)\n",
    "            action_step = policy.action(time_step)\n",
    "            current_action = action_step.action\n",
    "\n",
    "            generated_experience.append({\n",
    "                'state': current_state[0].numpy(),\n",
    "                'action': current_action,\n",
    "                'reward': reward[0][0].numpy(),\n",
    "                'next_state': next_state[0].numpy(),\n",
    "            })\n",
    "            \n",
    "            # Update current state\n",
    "            current_state = next_state\n",
    "            current_reward = reward\n",
    "\n",
    "\n",
    "\n",
    "            print(\"Current action shape: \", current_action.shape)\n",
    "            print(\"Current action: \", current_action)\n",
    "            # Generate next state and reward using world model\n",
    "            next_state, reward, uncertainty = self.world_model(\n",
    "                (current_state, current_action)\n",
    "            )\n",
    "            \n",
    "            print(\"Next state shape: \", next_state.shape)\n",
    "            print(\"Reward shape: \", reward.shape)\n",
    "            print(\"Uncertainty shape: \", uncertainty.shape)\n",
    "\n",
    "            # Check uncertainty threshold\n",
    "            if tf.reduce_mean(uncertainty) > self.uncertainty_threshold:\n",
    "                break\n",
    "            \n",
    "\n",
    "\n",
    "            # Store experience\n",
    "            generated_experience.append({\n",
    "                'state': current_state[0].numpy(),\n",
    "                'action': current_action,\n",
    "                'reward': reward[0][0].numpy(),\n",
    "                'next_state': next_state[0].numpy(),\n",
    "            })\n",
    "            \n",
    "            # Update current state\n",
    "            current_state = next_state\n",
    "            current_reward = reward\n",
    "            \n",
    "        return generated_experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to train the world model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_world_model(world_model, replay_buffer, batch_size=256, training_steps=1):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    \n",
    "    for step in range(training_steps):\n",
    "        # Sample batch from replay buffer and iterate properly\n",
    "        print(\"Training world model step: \", step)\n",
    "        dataset = replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3,\n",
    "            sample_batch_size=batch_size,\n",
    "            num_steps=2\n",
    "        ).take(1)\n",
    "        \n",
    "        print(\"Dataset created\")\n",
    "\n",
    "        for experience_batch, _ in dataset:\n",
    "            print(\"Experience batch created\")\n",
    "            # Extract current states, actions, next states and rewards\n",
    "            states = experience_batch.observation\n",
    "            actions = experience_batch.action[:, 0, :]  # Take first timestep actions\n",
    "            next_states = experience_batch.observation[:, 1, :]  # Take second timestep states \n",
    "            rewards = experience_batch.reward[:, 0]  # Take first timestep rewards\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get predictions (remove time dimension for single step prediction)\n",
    "                pred_next_states, pred_rewards, _ = world_model(\n",
    "                    states[:, 0, :],  # Take first timestep states\n",
    "                    actions\n",
    "                )\n",
    "                \n",
    "                # Calculate losses\n",
    "                state_loss = tf.reduce_mean(tf.square(pred_next_states - next_states))\n",
    "                reward_loss = tf.reduce_mean(tf.square(pred_rewards - tf.expand_dims(rewards, -1)))\n",
    "                total_loss = state_loss + reward_loss\n",
    "            \n",
    "            # Update model\n",
    "            grads = tape.gradient(total_loss, world_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, world_model.trainable_variables))\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            logging_info(f'World Model Step {step}, Loss: {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_DN734lZAwE"
   },
   "source": [
    "Finally we're ready to execute the RL traiing loop with SAC!\n",
    "\n",
    "You can sepcify the total number of trainng iterations, and the number of gradient steps per iteration. With fewer steps, the model will train more slowly, but more steps may make the agent less stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "form",
    "id": "PAlT1f6SWYxq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Training iteration:  0\n",
      "initial states shape:  (53,)\n",
      "Current state shape:  (1, 53)\n",
      "Current state[0] shape:  (53,)\n",
      "np array shape:  (1, 53)\n",
      "Time step observation shape:  (1, 53)\n",
      "Time step reward:  0.0\n",
      "Time step discount:  1.0\n",
      "Current action shape:  (1, 2)\n",
      "Current action:  [[0.72826785 0.99817115]]\n",
      "Next state shape:  (1, 53, 53)\n",
      "Reward shape:  (1, 53, 1)\n",
      "Uncertainty shape:  (1, 53, 53)\n",
      "Current state shape:  (1, 53, 53)\n",
      "Current state[0] shape:  (53, 53)\n",
      "np array shape:  (1, 53, 53)\n",
      "Time step observation shape:  (1, 53, 53)\n",
      "Time step reward:  0.0\n",
      "Time step discount:  1.0\n",
      "Current action shape:  (1, 53, 2)\n",
      "Current action:  [[[-0.28525665 -0.7501123 ]\n",
      "  [-0.45385224 -0.5829287 ]\n",
      "  [ 0.43410608 -0.08825911]\n",
      "  [-0.7534623   0.55185175]\n",
      "  [-0.81793916  0.31511515]\n",
      "  [ 0.6387929  -0.900058  ]\n",
      "  [ 0.06714299 -0.20102036]\n",
      "  [-0.7456854   0.17453213]\n",
      "  [ 0.3943809  -0.6397639 ]\n",
      "  [-0.5726215   0.17962106]\n",
      "  [ 0.43580076 -0.67106885]\n",
      "  [-0.02138055  0.5834041 ]\n",
      "  [-0.5801684  -0.30993652]\n",
      "  [-0.31060344  0.06361432]\n",
      "  [-0.7472978  -0.37765202]\n",
      "  [-0.31399983 -0.91818416]\n",
      "  [ 0.28793117  0.8273    ]\n",
      "  [-0.7288333   0.72431684]\n",
      "  [-0.00517697  0.28253353]\n",
      "  [ 0.8820463  -0.781613  ]\n",
      "  [-0.5772702  -0.24341993]\n",
      "  [-0.646643   -0.6526521 ]\n",
      "  [-0.7191157   0.614281  ]\n",
      "  [ 0.57818794 -0.4324001 ]\n",
      "  [ 0.1754566   0.87280345]\n",
      "  [-0.9125948  -0.04637503]\n",
      "  [-0.9344088   0.5343092 ]\n",
      "  [-0.7159828  -0.5894204 ]\n",
      "  [ 0.18590245  0.94989985]\n",
      "  [ 0.6253628  -0.69023377]\n",
      "  [-0.05951742  0.67882556]\n",
      "  [-0.8096949   0.8013014 ]\n",
      "  [-0.7181246   0.75307757]\n",
      "  [-0.19261874 -0.7001989 ]\n",
      "  [-0.8720225  -0.6734669 ]\n",
      "  [ 0.63283986 -0.7140355 ]\n",
      "  [-0.5514671   0.5763346 ]\n",
      "  [-0.10515141 -0.90165555]\n",
      "  [-0.400917   -0.63731366]\n",
      "  [ 0.40413308 -0.66799897]\n",
      "  [ 0.28970182 -0.6987662 ]\n",
      "  [-0.6131227   0.6604915 ]\n",
      "  [-0.15765178  0.11925526]\n",
      "  [-0.32811174 -0.8527859 ]\n",
      "  [ 0.06098097  0.52536607]\n",
      "  [-0.2127206  -0.90832144]\n",
      "  [-0.17714253  0.1976094 ]\n",
      "  [-0.9221396  -0.8237404 ]\n",
      "  [-0.09359514  0.48132488]\n",
      "  [ 0.08742915  0.2631012 ]\n",
      "  [-0.58868676 -0.2928158 ]\n",
      "  [ 0.5787523  -0.50192344]\n",
      "  [-0.35901454  0.68766063]]]\n",
      "Next state shape:  (1, 53, 53)\n",
      "Reward shape:  (1, 53, 1)\n",
      "Uncertainty shape:  (1, 53, 53)\n",
      "Current state shape:  (1, 53, 53)\n",
      "Current state[0] shape:  (53, 53)\n",
      "np array shape:  (1, 53, 53)\n",
      "Time step observation shape:  (1, 53, 53)\n",
      "Time step reward:  0.0\n",
      "Time step discount:  1.0\n",
      "Current action shape:  (1, 53, 2)\n",
      "Current action:  [[[-0.6271854   0.5046014 ]\n",
      "  [-0.04173234  0.4403767 ]\n",
      "  [-0.19112886 -0.88660157]\n",
      "  [-0.4794223  -0.8991272 ]\n",
      "  [-0.54457814  0.7049938 ]\n",
      "  [ 0.10107998 -0.7427786 ]\n",
      "  [-0.12308218 -0.79661876]\n",
      "  [-0.44118083 -0.5594918 ]\n",
      "  [-0.54858893 -0.9418768 ]\n",
      "  [-0.43586767 -0.56029904]\n",
      "  [ 0.12403458 -0.95131814]\n",
      "  [ 0.2516522  -0.97852856]\n",
      "  [-0.33134416  0.9113094 ]\n",
      "  [ 0.5511696  -0.5145894 ]\n",
      "  [ 0.6195588  -0.05902229]\n",
      "  [-0.17887752  0.42068946]\n",
      "  [ 0.38142657  0.10389055]\n",
      "  [ 0.6252489  -0.87049866]\n",
      "  [-0.4148901  -0.81268615]\n",
      "  [-0.04052913 -0.4795679 ]\n",
      "  [ 0.00993805  0.8984733 ]\n",
      "  [-0.13322406 -0.6751699 ]\n",
      "  [ 0.33541873 -0.72786593]\n",
      "  [ 0.443184   -0.984408  ]\n",
      "  [ 0.4599381  -0.23954898]\n",
      "  [ 0.33181944 -0.40168902]\n",
      "  [ 0.63492674 -0.85946643]\n",
      "  [ 0.6928812  -0.5315973 ]\n",
      "  [ 0.55426544 -0.64554185]\n",
      "  [-0.03831072 -0.72978514]\n",
      "  [ 0.42872182  0.00222307]\n",
      "  [-0.4839015  -0.9843709 ]\n",
      "  [-0.42733827  0.13942406]\n",
      "  [ 0.2923255  -0.77807546]\n",
      "  [-0.34790495  0.10366932]\n",
      "  [ 0.7401409  -0.9060945 ]\n",
      "  [-0.757909   -0.27940568]\n",
      "  [ 0.7409565  -0.6450348 ]\n",
      "  [ 0.15162379  0.52911425]\n",
      "  [-0.27241132 -0.57908237]\n",
      "  [ 0.43704706 -0.9236586 ]\n",
      "  [ 0.1753304   0.8061198 ]\n",
      "  [-0.1725665  -0.6975942 ]\n",
      "  [ 0.12089425 -0.8401151 ]\n",
      "  [ 0.09450495 -0.1331141 ]\n",
      "  [-0.41027418  0.7071709 ]\n",
      "  [-0.08533979 -0.8715867 ]\n",
      "  [ 0.5367932  -0.6154469 ]\n",
      "  [ 0.19146642 -0.8880311 ]\n",
      "  [-0.17569862 -0.86750555]\n",
      "  [ 0.14952268  0.12841827]\n",
      "  [ 0.10496386 -0.8820428 ]\n",
      "  [ 0.48720467 -0.52845895]]]\n",
      "Next state shape:  (1, 53, 53)\n",
      "Reward shape:  (1, 53, 1)\n",
      "Uncertainty shape:  (1, 53, 53)\n",
      "Current state shape:  (1, 53, 53)\n",
      "Current state[0] shape:  (53, 53)\n",
      "np array shape:  (1, 53, 53)\n",
      "Time step observation shape:  (1, 53, 53)\n",
      "Time step reward:  0.0\n",
      "Time step discount:  1.0\n",
      "Current action shape:  (1, 53, 2)\n",
      "Current action:  [[[-0.4622094  -0.9521298 ]\n",
      "  [ 0.7124245  -0.86822385]\n",
      "  [-0.4139332  -0.5907512 ]\n",
      "  [-0.52591115 -0.74858993]\n",
      "  [ 0.18583025 -0.85209876]\n",
      "  [-0.2991341  -0.78603613]\n",
      "  [ 0.5961461  -0.93867826]\n",
      "  [ 0.3597598  -0.69356835]\n",
      "  [-0.68165016 -0.49133107]\n",
      "  [-0.2909581  -0.6770144 ]\n",
      "  [-0.6992007  -0.59169406]\n",
      "  [-0.284609   -0.9722502 ]\n",
      "  [-0.2688183  -0.6292765 ]\n",
      "  [-0.8158596   0.5210542 ]\n",
      "  [ 0.3074586  -0.98737425]\n",
      "  [-0.9679278  -0.96626544]\n",
      "  [ 0.1867224  -0.35323405]\n",
      "  [ 0.618634   -0.9922019 ]\n",
      "  [-0.1284846  -0.9656696 ]\n",
      "  [ 0.6472287  -0.9997035 ]\n",
      "  [-0.06864063 -0.36993015]\n",
      "  [-0.66416687 -0.9868252 ]\n",
      "  [-0.8601093  -0.8801816 ]\n",
      "  [ 0.10810614 -0.61307573]\n",
      "  [-0.1536624  -0.7933904 ]\n",
      "  [ 0.05567541 -0.7077005 ]\n",
      "  [-0.06888127 -0.42740306]\n",
      "  [-0.03071336 -0.76194894]\n",
      "  [-0.6557591  -0.89212364]\n",
      "  [ 0.18205883 -0.6131056 ]\n",
      "  [-0.67751235 -0.9786836 ]\n",
      "  [ 0.75260854 -0.8253185 ]\n",
      "  [-0.97415924 -0.11315422]\n",
      "  [-0.96490246 -0.9804536 ]\n",
      "  [ 0.75498223 -0.4981078 ]\n",
      "  [-0.61150676 -0.9951564 ]\n",
      "  [ 0.02396286 -0.87142724]\n",
      "  [-0.8229591  -0.8096711 ]\n",
      "  [ 0.55872655 -0.9206141 ]\n",
      "  [ 0.9105681  -0.9689339 ]\n",
      "  [-0.22586939 -0.44541422]\n",
      "  [ 0.69373155 -0.42201662]\n",
      "  [ 0.43879515 -0.36863488]\n",
      "  [ 0.4091715  -0.9255905 ]\n",
      "  [-0.05549734 -0.75379413]\n",
      "  [ 0.48047262  0.30494913]\n",
      "  [ 0.29939345 -0.91315895]\n",
      "  [ 0.9655838  -0.86924964]\n",
      "  [ 0.60525084 -0.9415454 ]\n",
      "  [-0.07917541 -0.38849804]\n",
      "  [ 0.6303593  -0.7928284 ]\n",
      "  [-0.09050171 -0.8790935 ]\n",
      "  [ 0.05258143 -0.9230716 ]]]\n",
      "Next state shape:  (1, 53, 53)\n",
      "Reward shape:  (1, 53, 1)\n",
      "Uncertainty shape:  (1, 53, 53)\n",
      "Current state shape:  (1, 53, 53)\n",
      "Current state[0] shape:  (53, 53)\n",
      "np array shape:  (1, 53, 53)\n",
      "Time step observation shape:  (1, 53, 53)\n",
      "Time step reward:  0.0\n",
      "Time step discount:  1.0\n",
      "Current action shape:  (1, 53, 2)\n",
      "Current action:  [[[-0.6868594  -0.1908066 ]\n",
      "  [-0.73276573 -0.6133078 ]\n",
      "  [-0.79986465 -0.7090506 ]\n",
      "  [ 0.6874111  -0.8771028 ]\n",
      "  [ 0.77714264 -0.88967633]\n",
      "  [-0.82092327  0.20609695]\n",
      "  [ 0.12080945 -0.15913957]\n",
      "  [ 0.5199838  -0.9228764 ]\n",
      "  [ 0.90263426 -0.08294845]\n",
      "  [ 0.5850674  -0.20636494]\n",
      "  [ 0.50851846  0.00296848]\n",
      "  [ 0.8509511  -0.6625042 ]\n",
      "  [ 0.04666329  0.4075774 ]\n",
      "  [-0.09194368 -0.9850742 ]\n",
      "  [-0.04294686  0.02402898]\n",
      "  [-0.03388787  0.12646653]\n",
      "  [-0.24489929 -0.83488804]\n",
      "  [ 0.6914094  -0.4164341 ]\n",
      "  [-0.36636958 -0.9382631 ]\n",
      "  [ 0.10581607 -0.07551491]\n",
      "  [-0.38912994 -0.4832405 ]\n",
      "  [ 0.5360789  -0.41590312]\n",
      "  [ 0.21247128 -0.9172001 ]\n",
      "  [ 0.56001335 -0.86153257]\n",
      "  [ 0.0884206  -0.60397357]\n",
      "  [ 0.5606147  -0.8528332 ]\n",
      "  [-0.00863948 -0.7533573 ]\n",
      "  [ 0.6569024   0.02445854]\n",
      "  [ 0.23252346  0.7139632 ]\n",
      "  [-0.24222404 -0.44111353]\n",
      "  [ 0.14119717 -0.8046159 ]\n",
      "  [ 0.4480853  -0.8327899 ]\n",
      "  [ 0.27861336 -0.49464226]\n",
      "  [ 0.8539116  -0.3468004 ]\n",
      "  [ 0.0997152  -0.8480755 ]\n",
      "  [ 0.4131283   0.04678749]\n",
      "  [ 0.4412284   0.2977775 ]\n",
      "  [ 0.4009547  -0.77742136]\n",
      "  [-0.41825235 -0.9778468 ]\n",
      "  [-0.5135073   0.12221865]\n",
      "  [-0.39123315 -0.5025036 ]\n",
      "  [-0.04077454 -0.55442303]\n",
      "  [ 0.7019436  -0.06548437]\n",
      "  [ 0.5248228   0.0040733 ]\n",
      "  [-0.00347544 -0.7072185 ]\n",
      "  [ 0.2764938  -0.50197357]\n",
      "  [-0.3166708  -0.0919162 ]\n",
      "  [ 0.64662075  0.1437548 ]\n",
      "  [ 0.35071263 -0.9927103 ]\n",
      "  [ 0.08050634 -0.9180934 ]\n",
      "  [ 0.50000477  0.5763589 ]\n",
      "  [-0.00607319 -0.5562351 ]\n",
      "  [ 0.89359534 -0.1358134 ]]]\n",
      "Next state shape:  (1, 53, 53)\n",
      "Reward shape:  (1, 53, 1)\n",
      "Uncertainty shape:  (1, 53, 53)\n",
      "Generated experience length:  5\n",
      "{'state': array([-20.        , -19.128613  ,   0.        ,  -0.82162595,\n",
      "        -0.5831852 ,   4.8570604 ,   0.        ,  -2.425231  ,\n",
      "        -1.1068594 ,   0.        ,   2.564033  ,   1.4996233 ,\n",
      "         1.        ,   0.        ,   0.        ,   0.        ,\n",
      "         0.        ,   0.        ,   1.        ,   0.        ,\n",
      "         0.        ,   0.        ,   0.        ,   0.        ,\n",
      "         0.        ,   0.        ,   0.        ,   1.        ,\n",
      "         0.        ,   0.        ,   0.        ,   0.        ,\n",
      "         0.        ,   0.        ,   0.        ,   0.        ,\n",
      "         0.        ,   0.        ,   0.        ,   0.        ,\n",
      "         0.        ,   0.        ,   0.        ,   0.        ,\n",
      "         0.        ,   0.        ,  -0.25881904,   0.9659258 ,\n",
      "        -0.90096885,   0.43388373,   0.        ,   0.        ,\n",
      "        -0.99206346], dtype=float32), 'action': array([[0.72826785, 0.99817115]], dtype=float32), 'reward': array([-1.0143727], dtype=float32), 'next_state': array([[ 1.2007082 ,  0.22959574, -0.17672335, ..., -1.7811346 ,\n",
      "         0.07735879,  0.5433257 ],\n",
      "       [ 1.2137134 ,  0.23854709, -0.20298424, ..., -1.8087438 ,\n",
      "         0.0971165 ,  0.5601016 ],\n",
      "       [ 1.2049428 ,  0.2510262 , -0.20243664, ..., -1.7614084 ,\n",
      "         0.1221164 ,  0.60990345],\n",
      "       ...,\n",
      "       [ 1.2090454 ,  0.23045951, -0.19923623, ..., -1.8012435 ,\n",
      "         0.08576491,  0.55212474],\n",
      "       [ 1.2090454 ,  0.23045951, -0.19923623, ..., -1.8012435 ,\n",
      "         0.08576491,  0.55212474],\n",
      "       [ 1.2090454 ,  0.23045951, -0.19923623, ..., -1.8012435 ,\n",
      "         0.08576491,  0.55212474]], dtype=float32)}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected first dimension to be 1, but saw outer dim: 53",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m generated_experience:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(exp)\n\u001b[0;32m---> 68\u001b[0m     rb_observer(\u001b[43mtrajectory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     75\u001b[0m loss_info \u001b[38;5;241m=\u001b[39m agent_learner\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m     76\u001b[0m     iterations\u001b[38;5;241m=\u001b[39mnum_gradient_updates_per_training_iteration\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m logging_info(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActor Loss: \u001b[39m\u001b[38;5;132;01m%6.2f\u001b[39;00m\u001b[38;5;124m, Critic Loss: \u001b[39m\u001b[38;5;132;01m%6.2f\u001b[39;00m\u001b[38;5;124m, Alpha Loss: \u001b[39m\u001b[38;5;132;01m%6.2f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m )\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tf_agents/trajectories/trajectory.py:607\u001b[0m, in \u001b[0;36mfrom_episode\u001b[0;34m(observation, action, policy_info, reward, discount)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_episode(observation, action, policy_info, reward, discount)\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tf_agents/trajectories/trajectory.py:580\u001b[0m, in \u001b[0;36mfrom_episode.<locals>._from_episode\u001b[0;34m(observation, action, policy_info, reward, discount)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(outer_dim) \u001b[38;5;129;01mand\u001b[39;00m outer_dim \u001b[38;5;241m!=\u001b[39m num_frames:\n\u001b[1;32m    574\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected first dimension to be \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, but saw outer dim: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    576\u001b[0m               num_frames, outer_dim\n\u001b[1;32m    577\u001b[0m           )\n\u001b[1;32m    578\u001b[0m       )\n\u001b[0;32m--> 580\u001b[0m   \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_num_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m ts_first \u001b[38;5;241m=\u001b[39m convert_fn(ts\u001b[38;5;241m.\u001b[39mStepType\u001b[38;5;241m.\u001b[39mFIRST)\n\u001b[1;32m    587\u001b[0m ts_last \u001b[38;5;241m=\u001b[39m convert_fn(ts\u001b[38;5;241m.\u001b[39mStepType\u001b[38;5;241m.\u001b[39mLAST)\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:631\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1066\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1068\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1106\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1108\u001b[0m )\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1106\u001b[0m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1108\u001b[0m )\n",
      "File \u001b[0;32m~/sbsim_model_based/.venv/lib/python3.10/site-packages/tf_agents/trajectories/trajectory.py:574\u001b[0m, in \u001b[0;36mfrom_episode.<locals>._from_episode.<locals>.check_num_frames\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    572\u001b[0m   outer_dim \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(outer_dim) \u001b[38;5;129;01mand\u001b[39;00m outer_dim \u001b[38;5;241m!=\u001b[39m num_frames:\n\u001b[0;32m--> 574\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected first dimension to be \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, but saw outer dim: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    576\u001b[0m           num_frames, outer_dim\n\u001b[1;32m    577\u001b[0m       )\n\u001b[1;32m    578\u001b[0m   )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected first dimension to be 1, but saw outer dim: 53"
     ]
    }
   ],
   "source": [
    "# @title Execute the training loop\n",
    "\n",
    "num_training_iterations = 10\n",
    "num_gradient_updates_per_training_iteration = 100\n",
    "\n",
    "# Collect the performance results with teh untrained model.\n",
    "#eval_actor.run_and_log()\n",
    "\n",
    "logging_info('Training.')\n",
    "\n",
    "world_model = TransformerWorldModel(\n",
    "    state_dim=collect_env.observation_spec().shape[0],\n",
    "    action_dim=collect_env.action_spec().shape[0],\n",
    "    hidden_dim=256,\n",
    "    num_heads=8,\n",
    "    num_transformer_blocks=3\n",
    ")\n",
    "\n",
    "# world_model = SimpleWorldModel(\n",
    "#     state_dim=collect_env.observation_spec().shape[0],\n",
    "#     action_dim=collect_env.action_spec().shape[0],\n",
    "#     hidden_dim=256\n",
    "# )\n",
    "\n",
    "model_buffer = ModelAssistedReplayBuffer(\n",
    "    real_buffer=reverb_replay,\n",
    "    world_model=world_model,\n",
    "    rollout_length=5,\n",
    "    rollout_ratio=0.5,\n",
    "    uncertainty_threshold=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# log_dir = root_dir + '/train'\n",
    "# with tf.summary.create_file_writer(log_dir).as_default() as writer:   \n",
    "\n",
    "for i in range(num_training_iterations):\n",
    "    print('Training iteration: ', i)\n",
    "    \n",
    "    #collect_actor.run()\n",
    "    \n",
    "    # train_world_model(\n",
    "    #     world_model=world_model,\n",
    "    #     replay_buffer=reverb_replay,\n",
    "    #     batch_size=256,\n",
    "    #     training_steps=1000\n",
    "    # )\n",
    "\n",
    "    # real_batch = next(iter(reverb_replay.as_dataset(\n",
    "    #     num_parallel_calls=3,\n",
    "    #     sample_batch_size=64,\n",
    "    #     num_steps=2\n",
    "    # )))\n",
    "    # print(\"Real batch created\")\n",
    "    \n",
    "\n",
    "    initial_states = ts.observation\n",
    "    initial_reward = ts.reward\n",
    "    initial_discount = ts.discount\n",
    "    print(\"initial states shape: \", initial_states.shape)\n",
    "    \n",
    "    generated_experience = model_buffer.generate_rollouts(\n",
    "        initial_states,\n",
    "        initial_reward,\n",
    "        initial_discount,\n",
    "        agent_collect_policy\n",
    "    )\n",
    "    print(\"Generated experience length: \", len(generated_experience))\n",
    "\n",
    "    for exp in generated_experience:\n",
    "        print(exp)\n",
    "        rb_observer(trajectory.from_episode(\n",
    "            observation=exp['state'],\n",
    "            action=exp['action'],\n",
    "            policy_info=(),\n",
    "            reward=exp['reward']\n",
    "        ))\n",
    "    \n",
    "    loss_info = agent_learner.run(\n",
    "        iterations=num_gradient_updates_per_training_iteration\n",
    "    )\n",
    "    \n",
    "    logging_info(\n",
    "        'Actor Loss: %6.2f, Critic Loss: %6.2f, Alpha Loss: %6.2f'\n",
    "        % (\n",
    "            loss_info.extra.actor_loss.numpy(),\n",
    "            loss_info.extra.critic_loss.numpy(),\n",
    "            loss_info.extra.alpha_loss.numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    eval_env.reset()\n",
    "    eval_actor.run_and_log()\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "L7w-mjPcH7u6",
    "kTtVb9wbRsKU",
    "86IIF7FrfJ_2",
    "SDgizVLzRti1"
   ],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1a2nzc-VcaGRTpsEFj3FgqRZY0Lk1dgrW",
     "timestamp": 1705074752110
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
